{
    "version": "https://jsonfeed.org/version/1",
    "title": "意大利炮打友军 • All posts by \"大模型\" category",
    "description": "",
    "home_page_url": "http://example.com",
    "items": [
        {
            "id": "http://example.com/2024/03/30/DeepSpeed%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B9%8B%EF%BC%9Aelasticity/",
            "url": "http://example.com/2024/03/30/DeepSpeed%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B9%8B%EF%BC%9Aelasticity/",
            "title": "DeepSpeed代码阅读笔记之：elasticity",
            "date_published": "2024-03-30T12:54:57.000Z",
            "content_html": "<p>本笔记是DeepSpeed代码阅读的第一篇笔记，本周的主要任务是阅读DeepSpeed python代码中的</p>\n<h2 id=\"DeepSpeed-部署\"><a href=\"#DeepSpeed-部署\" class=\"headerlink\" title=\"DeepSpeed 部署\"></a>DeepSpeed 部署</h2><p>DeepSpeed 部署的过程如下：</p>\n<ol>\n<li>安装cuda与pytorch</li>\n<li>按照requirements文件夹安装依赖：</li>\n</ol>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><div class=\"code-wrapper\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></div></td><td class=\"code\"><pre><code class=\"hljs bash\">pip install -r requirements/requirements-dev.txt <br>pip install -r requirements/requirements.txt<br>pip install -r requirements/requirements-sparse_attn.txt <br>pip install mpi4py<br>pip install --ignore-installed PyYAML<br></code></pre></td></tr></table></figure>\n<p><strong>注意：attn文件里的triton 可能没有1.0版本</strong><br><strong>mpi4py可能需要通过conda安装</strong><br>3. 安装DeepSpeed:运行install.sh<br><strong>注意：deepspeed需要全目录有rw权限</strong></p>\n<p>至此，安装已完成，可以使用DeepSpeedExample目录下的例程来测试。</p>\n<h2 id=\"elasticity目录代码\"><a href=\"#elasticity目录代码\" class=\"headerlink\" title=\"elasticity目录代码\"></a>elasticity目录代码</h2><p>init中说明了本目录下有如下几个文件：</p>\n<figure class=\"highlight ada\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs ada\"><span class=\"hljs-comment\">--elasticity</span><br>    |<span class=\"hljs-comment\">-- __init__.py</span><br>    |<span class=\"hljs-comment\">-- elasticity.py</span><br>    |<span class=\"hljs-comment\">-- utils.py</span><br>    |<span class=\"hljs-comment\">-- contants.py</span><br>    |<span class=\"hljs-comment\">-- elastic_agent.py</span><br>    |<span class=\"hljs-comment\">-- config.py</span><br></code></pre></td></tr></table></figure>\n\n<h2 id=\"init-py\"><a href=\"#init-py\" class=\"headerlink\" title=\"_init_.py\"></a>_<em>init_</em>.py</h2><p>这个代码主要是把目录下的文件中包含外部接口的文件进行引用，需要判断pytorch版本是否大于1.11，只有高于此版本才能使用DSElasticAgent类</p>\n<h2 id=\"elasticity\"><a href=\"#elasticity\" class=\"headerlink\" title=\"elasticity\"></a>elasticity</h2><p>elasticity.py中给出了几个接口函数用来供外部调用</p>\n<h3 id=\"compute-elastic-config\"><a href=\"#compute-elastic-config\" class=\"headerlink\" title=\"compute_elastic_config\"></a>compute_elastic_config</h3><p>调用弹性计算的核心代码，在DeepSpeedConfig类中会检查config是否有配置弹性计算，如果有会调用这个函数最终得到总batch——size和根据当前可用GPU数量得到的micro_batch(用于数据并行)（可选）</p>\n<p>elasticity 0.1版本和0.2版本分别调用不同的函数来得到final_batch_size。</p>\n<p>最后，通过检查micro_batch_size能否在数据并行中和batch_size 匹配(batch_size分在每个GPU上的大小能为micro_batch_size整倍)</p>\n<p><strong>感觉这部分代码的去耦合做的很不好</strong></p>\n<h3 id=\"elasticity-enabled\"><a href=\"#elasticity-enabled\" class=\"headerlink\" title=\"elasticity_enabled\"></a>elasticity_enabled</h3><p>检查ds_config的ELSASTICITY是否启用</p>\n<h3 id=\"ensure-immutable-elastic-config\"><a href=\"#ensure-immutable-elastic-config\" class=\"headerlink\" title=\"ensure_immutable_elastic_config\"></a>ensure_immutable_elastic_config</h3><p>确保在资源管理器启用的情况下，根据环境变量中给Deepspeed的弹性配置和deepspeed自身的配置来检查是否匹配。</p>\n<p>除此之外，elasticity中内部还有关于micro_batch_size和total_batch_size计算实现的代码</p>\n<h3 id=\"get-compatible-gpus-v01\"><a href=\"#get-compatible-gpus-v01\" class=\"headerlink\" title=\"_get_compatible_gpus_v01\"></a>_get_compatible_gpus_v01</h3><p>这个函数主要是得到batch_size和可用gpu数量。首先按照指定的micro_batches得到候选的batch_size。这个过程是给定的mrbs来找出最大合适的batch_size，存储在列表里。<br>在此之后，通过batch_sized候选列表中，按照偏好（大or小batch）得到最好的合适的（满足GPU数量和偏好）的batch_size。</p>\n<figure class=\"highlight stata\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs stata\">注意：这里满足GPU数量是指mrbs可以被<span class=\"hljs-keyword\">bs</span>整除，同时在给定最大or最小GPU数量中。<br></code></pre></td></tr></table></figure>\n\n<h3 id=\"get-compatible-gpus-v02\"><a href=\"#get-compatible-gpus-v02\" class=\"headerlink\" title=\"_get_compatible_gpus_v02\"></a>_get_compatible_gpus_v02</h3><p>在v01函数的基础上，这个函数根据bs的大小和GPU，根据节点的GPU数量来调整mrbs数量。以及根据数据并行dpsz来得到bs&#x3D;bs*dpsz。</p>\n<h2 id=\"utils\"><a href=\"#utils\" class=\"headerlink\" title=\"utils\"></a>utils</h2><p>仅负责检查torch版本是否匹配</p>\n<h2 id=\"constant\"><a href=\"#constant\" class=\"headerlink\" title=\"constant\"></a>constant</h2><p>存储了必需的常数，类似ENABLE，DS最低版本，环境变量名。默认bs等。</p>\n<h2 id=\"elastic-agent\"><a href=\"#elastic-agent\" class=\"headerlink\" title=\"elastic_agent\"></a>elastic_agent</h2><p>实现了pytorch LocalElasticAgent的子类。</p>\n<h3 id=\"set-master-addr-port\"><a href=\"#set-master-addr-port\" class=\"headerlink\" title=\"_set_master_addr_port\"></a>_set_master_addr_port</h3><p>这个方法检查主节点的地址（端口），如果没有会自动生成</p>\n<h3 id=\"start-workers\"><a href=\"#start-workers\" class=\"headerlink\" title=\"_start_workers\"></a>_start_workers</h3><p>这个方法使用torch distributed的WorkerGroup类作为参数，给每个worker设定必要的环境变量后，给关于本地worker数量的环境变量进行更新。同时指定必要的参数后，启动workers的进程。</p>\n<h3 id=\"invoke-run\"><a href=\"#invoke-run\" class=\"headerlink\" title=\"_invoke_run\"></a>_invoke_run</h3><p>这个方法在worker启动后，每隔一段时间监控当前workerGroup的状态。<br>可能会遇到节点工作失败的状况，则会选择进行重启worker或者在全部失效时进行报错推出<br>在遇到节点加入或退出时，会进行记录并重启workers。</p>\n<h2 id=\"config-py\"><a href=\"#config-py\" class=\"headerlink\" title=\"config.py\"></a>config.py</h2><p>这个文件主要定义了和elasticity相关的错误抛出，以及对config从ds_config到elasticity_config参数的变换和类型检测。</p>\n<h2 id=\"问题\"><a href=\"#问题\" class=\"headerlink\" title=\"问题\"></a>问题</h2><ol>\n<li>为什么在elasticity文件中有预定义的HCN_LIST</li>\n<li>DS相比pytorch的Elasticity多了什么功能？</li>\n</ol>\n",
            "tags": [
                "技术",
                "大模型训练",
                "课题组",
                "笔记"
            ]
        },
        {
            "id": "http://example.com/2024/03/26/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%80%9A%E4%BF%A1%E7%AC%94%E8%AE%B02/",
            "url": "http://example.com/2024/03/26/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%80%9A%E4%BF%A1%E7%AC%94%E8%AE%B02/",
            "title": "大模型通信笔记2",
            "date_published": "2024-03-26T11:54:58.000Z",
            "content_html": "<h2 id=\"数据并行\"><a href=\"#数据并行\" class=\"headerlink\" title=\"数据并行\"></a>数据并行</h2><p>传统的数据并行是让每个GPU分别进行FWD和BWD，然后把梯度进行聚合操作，然后再下发给每个GPU，称为All Reduce。</p>\n<h3 id=\"缺点\"><a href=\"#缺点\" class=\"headerlink\" title=\"缺点\"></a>缺点</h3><ul>\n<li>存储开销大。每块GPU上都存了一份完整的模型，造成冗余</li>\n<li>通讯开销大。Server需要和每一个Worker进行梯度传输。当Server和Worker不在一台机器上时，Server的带宽将会成为整个系统的计算效率瓶颈。</li>\n</ul>\n<h3 id=\"异步梯度更新\"><a href=\"#异步梯度更新\" class=\"headerlink\" title=\"异步梯度更新\"></a>异步梯度更新</h3><ul>\n<li>Worker不等待梯度更新，用旧的参数进行下一轮训练，可能会延迟一步更新梯度，整体收敛速度变慢，但是提升通讯计算比。</li>\n<li>延迟步数会指定</li>\n</ul>\n<h3 id=\"分布式数据并行\"><a href=\"#分布式数据并行\" class=\"headerlink\" title=\"分布式数据并行\"></a>分布式数据并行</h3><p>核心目标是降低通信压力，因此要将Server的通信压力转到Worker上，最简单的就是Ring-AllReduce。</p>\n<h4 id=\"Ring-Allreduce\"><a href=\"#Ring-Allreduce\" class=\"headerlink\" title=\"Ring Allreduce\"></a>Ring Allreduce</h4><p>核心思路是实现Reduce Scatter和All-Gather。GPU每次之和前后两个GPU通信，1卡给2卡发a号数据，2给3发b号，以此类推。三次更新后每张卡都有1个号的完整的数据。</p>\n<p>之后在进行All-Gather，依旧环形通信，把每个部分全聚合的都发给下一个，然后依此类推，3轮通信就可以覆盖所有。</p>\n<h2 id=\"显存开销\"><a href=\"#显存开销\" class=\"headerlink\" title=\"显存开销\"></a>显存开销</h2><p>数据并行中，每个卡都存储了所有参数，怎么办？</p>\n<p>在实际存储中，分为两部分存储：</p>\n<ul>\n<li>模型状态：包括参数，优化器，梯度等</li>\n<li>驻留数据：包括activation，碎片内存和缓冲区等。</li>\n</ul>\n<h2 id=\"优化措施\"><a href=\"#优化措施\" class=\"headerlink\" title=\"优化措施\"></a>优化措施</h2><h3 id=\"混合精度训练\"><a href=\"#混合精度训练\" class=\"headerlink\" title=\"混合精度训练\"></a>混合精度训练</h3><p>对于参数，activation，梯度，都使用fp16，对于参数（多存一份）和优化器使用fp32。<br>模型必存数据为$K\\phi$,那么最终总存储数据为$K\\phi + 4\\phi$<br>实际上，activation大小和batch有关，而且是可以抛弃的。</p>\n<h3 id=\"ZeRO-DP\"><a href=\"#ZeRO-DP\" class=\"headerlink\" title=\"ZeRO-DP\"></a>ZeRO-DP</h3><h4 id=\"第一步：优化器分割\"><a href=\"#第一步：优化器分割\" class=\"headerlink\" title=\"第一步：优化器分割\"></a>第一步：优化器分割</h4><p>每张卡只存储一部分优化器参数，在数据并行中，先通过AllReduce得到完整梯度，每个卡都更新自己的一部分梯度和参数，然后再AllGather。产生单卡通讯量$\\phi$。</p>\n<h4 id=\"第二步：梯度分割\"><a href=\"#第二步：梯度分割\" class=\"headerlink\" title=\"第二步：梯度分割\"></a>第二步：梯度分割</h4><p>经过FWD和BWD后，对梯度进行Reduce-Scatter，保证每张卡都有自己一份聚合梯度，用分割的优化器和梯度进行更新相应的W，然后再AllGather参数进行更新</p>\n<h4 id=\"第三步：参数分割\"><a href=\"#第三步：参数分割\" class=\"headerlink\" title=\"第三步：参数分割\"></a>第三步：参数分割</h4><p>FWD时，先All Gather一次参数，用完即弃。<br>BWD时，再All Gather一次参数，用完即弃<br>用自己的梯度进行一次All Gather得到完整梯度<br>更新参数，无需通信。</p>\n<h3 id=\"ZeRO-R\"><a href=\"#ZeRO-R\" class=\"headerlink\" title=\"ZeRO-R\"></a>ZeRO-R</h3><p>通过对驻留数据进行优化来实现显存使用减少和通信负载降低。</p>\n<h4 id=\"activation\"><a href=\"#activation\" class=\"headerlink\" title=\"activation\"></a>activation</h4><p>每块GPU上只维护部分的activation，需要时再聚合。或者重新计算。</p>\n<h4 id=\"Buffer\"><a href=\"#Buffer\" class=\"headerlink\" title=\"Buffer\"></a>Buffer</h4><p>通过使用固定大小的Buffer，降低通信次数，减少碎片信息发送，提高带宽利用率</p>\n<h4 id=\"碎片内存整合\"><a href=\"#碎片内存整合\" class=\"headerlink\" title=\"碎片内存整合\"></a>碎片内存整合</h4><h3 id=\"ZeRO-Offload\"><a href=\"#ZeRO-Offload\" class=\"headerlink\" title=\"ZeRO-Offload\"></a>ZeRO-Offload</h3><p>见论文，主要是把显存的优化器参数卸载到CPU内存。</p>\n",
            "tags": [
                "技术",
                "大模型训练",
                "博客",
                "通信"
            ]
        },
        {
            "id": "http://example.com/2024/03/26/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%80%9A%E4%BF%A1%E7%AC%94%E8%AE%B01/",
            "url": "http://example.com/2024/03/26/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%80%9A%E4%BF%A1%E7%AC%94%E8%AE%B01/",
            "title": "大模型通信笔记1",
            "date_published": "2024-03-26T09:38:32.000Z",
            "content_html": "<h2 id=\"流水线并行\"><a href=\"#流水线并行\" class=\"headerlink\" title=\"流水线并行\"></a>流水线并行</h2><h3 id=\"朴素层并行\"><a href=\"#朴素层并行\" class=\"headerlink\" title=\"朴素层并行\"></a>朴素层并行</h3><p>朴素层并行，将模型拆分为多个层，放在不同的GPU上执行<br>但是问题很明显：</p>\n<ul>\n<li>GPU利用率低：任意时刻只有一个GPU在工作，其他GPU都在等待结果</li>\n<li>计算和通信没有重叠</li>\n<li>显存占用高，GPU1需要保存所有激活。等待参数更新完成</li>\n</ul>\n<h3 id=\"GPipe\"><a href=\"#GPipe\" class=\"headerlink\" title=\"GPipe\"></a>GPipe</h3><p>Gpipe将整个<strong>minibatch分为4个microbatch</strong>，然后由GPU0进行计算，之后每个microbatch计算完直接传递给GPU1，以此类推，进行整个前向、反向传播。<br>假设pipeline深度n，microbatch数量m，那么浪费的时间占比为：<br>$$<br>1-\\frac{m}{m+n-1}<br>$$<br>所以需要增加microbatch数量m<br>Gpipe在计算过程中，把中间激活用完即弃，因此节省了显存，但是增加了计算代价。</p>\n<h3 id=\"PipeDream\"><a href=\"#PipeDream\" class=\"headerlink\" title=\"PipeDream\"></a>PipeDream</h3><p>PipeDream在GPipe的基础上，在每个microbatch前向结束后就开始反向传播，节省了一些显存，bubble和Gpipe是一样的</p>\n<h3 id=\"数据并行可以和流水线并行同时进行\"><a href=\"#数据并行可以和流水线并行同时进行\" class=\"headerlink\" title=\"数据并行可以和流水线并行同时进行\"></a>数据并行可以和流水线并行同时进行</h3><p>对任意给定GPU，有两个通信部份，一部分包含所有相同层GPU进行All_Reduce(数据并行)。另一部分和上下层进行通信（流水线）。</p>\n<h2 id=\"张量并行\"><a href=\"#张量并行\" class=\"headerlink\" title=\"张量并行\"></a>张量并行</h2><p>张量并行分为两种情况：<strong>列划分</strong>和<strong>行划分</strong><br>列划分：<br>$$<br>XA &#x3D; X[A_1,A_2···A_n]&#x3D;[XA_1,XA_2,···,XA_n]<br>$$</p>\n<p>行划分：</p>\n<p>$$<br>\\mathbf{x}*A &#x3D; \\begin{bmatrix} x_1 &amp; x_2 &amp; \\cdots &amp; x_n \\end{bmatrix} * \\begin{bmatrix}A_1\\A_2\\A_3\\··· \\A_n\\end{bmatrix}&#x3D;X_1A_1+X_2A_2+X_3A_3···<br>$$</p>\n<p>对列并行来说，由于GeLU函数并不是线性的，因此需要在输出前进行一次通信来合并。</p>\n<h3 id=\"2D并行\"><a href=\"#2D并行\" class=\"headerlink\" title=\"2D并行\"></a>2D并行</h3><p>具体来说，两个矩阵的结果仍然需要串行的计算。但是，单个矩阵中的4个子矩阵可以使用2*2的处理器来并行计算。</p>\n<h3 id=\"2-5D并行\"><a href=\"#2-5D并行\" class=\"headerlink\" title=\"2.5D并行\"></a>2.5D并行</h3><p>这个就是在2D并行的基础上，左矩阵为两个2*2矩阵垂直拼接，那么这两个矩阵是可以分开计算的，所以可以8处理器并行计算。</p>\n<h2 id=\"3D并行\"><a href=\"#3D并行\" class=\"headerlink\" title=\"3D并行\"></a>3D并行</h2><p>流水线+数据+张量并行</p>\n<p>首先，每个节点8个GPU，共两个节点</p>\n<p>8个GPU，分为两组，每组负责一个Layer，一共四个组进行流水线并行。<br>每个组内，用两张卡进行张量并行，一个组分为两个张量小组。一个张量小组负责一个具体的张量运算。<br>对于两个张量小组之间，分享同一个batch不同的数据，在计算结束后两个小组之间要进行all reduce通信。</p>\n",
            "tags": [
                "技术",
                "大模型训练",
                "博客",
                "通信"
            ]
        }
    ]
}