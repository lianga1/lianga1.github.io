{
    "version": "https://jsonfeed.org/version/1",
    "title": "意大利炮打友军 • All posts by \"技术\" tag",
    "description": "",
    "home_page_url": "http://example.com",
    "items": [
        {
            "id": "http://example.com/2023/11/15/%E8%AF%BE%E9%A2%98%E7%BB%8423-11-15%E5%91%A8%E6%8A%A5/",
            "url": "http://example.com/2023/11/15/%E8%AF%BE%E9%A2%98%E7%BB%8423-11-15%E5%91%A8%E6%8A%A5/",
            "title": "课题组23-11-15周报",
            "date_published": "2023-11-15T08:09:06.000Z",
            "content_html": "<h1 id=\"实验关于fp16参数转换速度的问题\"><a href=\"#实验关于fp16参数转换速度的问题\" class=\"headerlink\" title=\"实验关于fp16参数转换速度的问题\"></a>实验关于fp16参数转换速度的问题</h1><p>本周进行了一个实验，主要用于观察pytorch中对张量转移的各种方法的性能差异。</p>\n<h2 id=\"实验思路\"><a href=\"#实验思路\" class=\"headerlink\" title=\"实验思路\"></a>实验思路</h2><h3 id=\"几种不同的传输方向\"><a href=\"#几种不同的传输方向\" class=\"headerlink\" title=\"几种不同的传输方向\"></a>几种不同的传输方向</h3><ul>\n<li>cpu -&gt; cpu</li>\n<li>cpu -&gt; gpu</li>\n<li>gpu -&gt; cpu</li>\n<li>gpu -&gt; gpu<h3 id=\"几种不同的数据\"><a href=\"#几种不同的数据\" class=\"headerlink\" title=\"几种不同的数据\"></a>几种不同的数据</h3></li>\n<li>fp32 -&gt; fp32</li>\n<li>fp32 -&gt; fp16<br><code>调用half()函数，将fp32数据转换为fp16数据</code></li>\n<li>fp16 -&gt; fp16</li>\n<li>fp16 -&gt; fp32<br><code>调用float()函数，将fp16数据转换为fp32数据</code></li>\n</ul>\n<p><strong>目前第三、四种暂未测试</strong></p>\n<h3 id=\"几种不同的传输方式\"><a href=\"#几种不同的传输方式\" class=\"headerlink\" title=\"几种不同的传输方式\"></a>几种不同的传输方式</h3><ul>\n<li>copy_()</li>\n<li>to()</li>\n</ul>\n<h2 id=\"实验过程\"><a href=\"#实验过程\" class=\"headerlink\" title=\"实验过程\"></a>实验过程</h2><p>准备一个目的地矩阵，一个源矩阵组（100个）。分别用随机数初始化。<br>循环100次，每次都遍历整个矩阵组，传输至对应的目的地矩阵。<br>测量总时长，对不同情况进行比较<br>代码模板如下：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><div class=\"code-wrapper\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></div></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-keyword\">import</span> torch<br><span class=\"hljs-keyword\">import</span> time<br>tensor_cpu_1 = torch.rand(<span class=\"hljs-number\">1000</span>, <span class=\"hljs-number\">1000</span>)<br>tensor_gpu_1 = torch.rand(<span class=\"hljs-number\">1000</span>,<span class=\"hljs-number\">1000</span>).cuda()<br>tensor_cpu_2 = torch.rand(<span class=\"hljs-number\">1000</span>, <span class=\"hljs-number\">1000</span>,<span class=\"hljs-number\">1000</span>)<br>tensor_gpu_2 = torch.rand(<span class=\"hljs-number\">1000</span>,<span class=\"hljs-number\">1000</span>,<span class=\"hljs-number\">1000</span>).cuda()<br>time_sum = <span class=\"hljs-number\">0</span><br><span class=\"hljs-keyword\">for</span> j <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">range</span>(<span class=\"hljs-number\">100</span>):<br>    start = time.time()<br>    <span class=\"hljs-keyword\">for</span> i <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">range</span>(<span class=\"hljs-number\">100</span>):<br>        tensor_cpu_1.copy_(tensor_cpu_2[i])<br>    end = time.time()<br>    time_sum += end - start<br><br><br><span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">&quot;time for copy from cpu to cpu via _copy():&quot;</span>, time_sum)<br><br></code></pre></td></tr></table></figure>\n<p>如上代码展示了从cpu到cpu传输fp32的过程。最终展示了传输十万个1000*1000的矩阵所耗费的总时间。<br>经过实验，结果如下表所示：<br>记录数据如下：</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">传输方向</th>\n<th align=\"center\">传输方式</th>\n<th align=\"center\">数据类型</th>\n<th align=\"center\">时间</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\">cpu -&gt; cpu</td>\n<td align=\"center\">copy_()</td>\n<td align=\"center\">fp32 -&gt; fp32</td>\n<td align=\"center\">2.187</td>\n</tr>\n<tr>\n<td align=\"center\">cpu -&gt; cpu</td>\n<td align=\"center\">to()</td>\n<td align=\"center\">fp32 -&gt; fp32</td>\n<td align=\"center\">0.025</td>\n</tr>\n<tr>\n<td align=\"center\">cpu -&gt; cpu</td>\n<td align=\"center\">copy_()</td>\n<td align=\"center\">fp32 -&gt; fp16</td>\n<td align=\"center\">5.855</td>\n</tr>\n<tr>\n<td align=\"center\">cpu -&gt; cpu</td>\n<td align=\"center\">to()</td>\n<td align=\"center\">fp32 -&gt; fp16</td>\n<td align=\"center\">5.634</td>\n</tr>\n<tr>\n<td align=\"center\">cpu -&gt; gpu</td>\n<td align=\"center\">copy_()</td>\n<td align=\"center\">fp32 -&gt; fp32</td>\n<td align=\"center\">9.663</td>\n</tr>\n<tr>\n<td align=\"center\">cpu -&gt; gpu</td>\n<td align=\"center\">to()</td>\n<td align=\"center\">fp32 -&gt; fp32</td>\n<td align=\"center\">9.555</td>\n</tr>\n<tr>\n<td align=\"center\">cpu -&gt; gpu</td>\n<td align=\"center\">copy_()</td>\n<td align=\"center\">fp32 -&gt; fp16</td>\n<td align=\"center\">9.876</td>\n</tr>\n<tr>\n<td align=\"center\">cpu -&gt; gpu</td>\n<td align=\"center\">to()</td>\n<td align=\"center\">fp32 -&gt; fp16</td>\n<td align=\"center\">10.264</td>\n</tr>\n<tr>\n<td align=\"center\">gpu -&gt; cpu</td>\n<td align=\"center\">copy_()</td>\n<td align=\"center\">fp32 -&gt; fp32</td>\n<td align=\"center\">8.895</td>\n</tr>\n<tr>\n<td align=\"center\">gpu -&gt; cpu</td>\n<td align=\"center\">to()</td>\n<td align=\"center\">fp32 -&gt; fp32</td>\n<td align=\"center\">13.649</td>\n</tr>\n<tr>\n<td align=\"center\">gpu -&gt; cpu</td>\n<td align=\"center\">copy_()</td>\n<td align=\"center\">fp32 -&gt; fp16</td>\n<td align=\"center\">10.051</td>\n</tr>\n<tr>\n<td align=\"center\">gpu -&gt; cpu</td>\n<td align=\"center\">to()</td>\n<td align=\"center\">fp32 -&gt; fp16</td>\n<td align=\"center\">7.320</td>\n</tr>\n<tr>\n<td align=\"center\">gpu -&gt; gpu</td>\n<td align=\"center\">copy_()</td>\n<td align=\"center\">fp32 -&gt; fp32</td>\n<td align=\"center\">0.605</td>\n</tr>\n<tr>\n<td align=\"center\">gpu -&gt; gpu</td>\n<td align=\"center\">to()</td>\n<td align=\"center\">fp32 -&gt; fp32</td>\n<td align=\"center\">0.029</td>\n</tr>\n<tr>\n<td align=\"center\">gpu -&gt; gpu</td>\n<td align=\"center\">copy_()</td>\n<td align=\"center\">fp32 -&gt; fp16</td>\n<td align=\"center\">0.484</td>\n</tr>\n<tr>\n<td align=\"center\">gpu -&gt; gpu</td>\n<td align=\"center\">to()</td>\n<td align=\"center\">fp32 -&gt; fp16</td>\n<td align=\"center\">0.591</td>\n</tr>\n</tbody></table>\n<ul>\n<li>注意，测试时间可能会有波动，尤其是在时间较短时，考虑到这种传输主要出现在gpu-&gt;gpu中，不是主要考虑内容</li>\n</ul>\n",
            "tags": [
                "技术",
                "周报",
                "课题组",
                "神经网络",
                "pytorch"
            ]
        },
        {
            "id": "http://example.com/2023/10/22/%E8%AF%BE%E9%A2%98%E7%BB%84%E7%AC%AC%E4%BA%94%E5%91%A8%E5%AD%A6%E4%B9%A0/",
            "url": "http://example.com/2023/10/22/%E8%AF%BE%E9%A2%98%E7%BB%84%E7%AC%AC%E4%BA%94%E5%91%A8%E5%AD%A6%E4%B9%A0/",
            "title": "课题组第五周学习",
            "date_published": "2023-10-22T15:36:49.000Z",
            "content_html": "<h1 id=\"高效直接访问主机内存的方法\"><a href=\"#高效直接访问主机内存的方法\" class=\"headerlink\" title=\"高效直接访问主机内存的方法\"></a>高效直接访问主机内存的方法</h1><h2 id=\"现有方法存在的问题\"><a href=\"#现有方法存在的问题\" class=\"headerlink\" title=\"现有方法存在的问题\"></a>现有方法存在的问题</h2><h3 id=\"通过加载后执行的方法\"><a href=\"#通过加载后执行的方法\" class=\"headerlink\" title=\"通过加载后执行的方法\"></a>通过加载后执行的方法</h3><p>面对巨大的模型参数规模，现有GPU的显存难以支撑大模型的训练。因此产生了一种通过加载后执行的方法，即将模型参数存储在主机内存中，每次训练时将参数加载到显存中，训练结束后将参数保存到主机内存中。这种方法的缺点是每次训练都需要将参数加载到显存中，这个过程会消耗大量的时间，例如在v100上，加载时间会是处理时间的4倍以上，导致训练效率低下。有一种异步加载方法，将加载层和训练层分开，训练层在训练时异步加载参数，但是这种方法会导致训练时的显存占用过高，而且层数较多时加载时间过高的劣势逐渐显现，优化并不明显。</p>\n<h2 id=\"本文提出的方法\"><a href=\"#本文提出的方法\" class=\"headerlink\" title=\"本文提出的方法\"></a>本文提出的方法</h2><h3 id=\"直接主机访问\"><a href=\"#直接主机访问\" class=\"headerlink\" title=\"直接主机访问\"></a>直接主机访问</h3><p>避开加载和训练不同步的问题，直接将cpu内存当作gpu的虚拟内存进行访问，这样避免了加载过程中占用gpu显存过高的问题，但是由于访问和数据流动要经过pcie总线，传输速度较慢。<br>因此，DHA使用了这样一种办法，使得其可以自适应选择访问方式，其可以通过直接主机访问和加载后执行两种方法进行训练，使得加载的时间可以隐藏在训练流的流水线中。</p>\n<h3 id=\"多GPU方法\"><a href=\"#多GPU方法\" class=\"headerlink\" title=\"多GPU方法\"></a>多GPU方法</h3><p>对于多个GPU，由于GPU间通信效率要高于PCIE通信效率，因此可以将模型拆分成多个部分，分别存储在不同的GPU中，这样每次训练的加载都可以直接从其他GPU中加载，而不需要从主机内存中加载，这样可以减少加载时间。</p>\n<h3 id=\"DeepPlan\"><a href=\"#DeepPlan\" class=\"headerlink\" title=\"DeepPlan\"></a>DeepPlan</h3><p>本文还提出了一个工具：用来为给定模型自动生成执行计划，过程如下：</p>\n<ul>\n<li>对本地GPU显存和主机内存分析性能</li>\n<li>通过比较DHA和流水线方法的延迟差异来决定每一层的策略</li>\n<li>如果有多个GPU，则根据GPU数量平均划分模型</li>\n<li>协调将直接主机访问的执行和加载后执行的执行进行协调<br>本方案在部署时只需要进行一次执行。<h2 id=\"原理分析\"><a href=\"#原理分析\" class=\"headerlink\" title=\"原理分析\"></a>原理分析</h2>对于不同层，加载——执行策略与DHA策略的时间是不同的，<table>\n<thead>\n<tr>\n<th align=\"center\">层</th>\n<th align=\"center\">加载——执行策略</th>\n<th align=\"center\">DHA策略</th>\n<th align=\"center\">结论</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\">卷积层</td>\n<td align=\"center\">小规模差异不大</td>\n<td align=\"center\">大规模较慢</td>\n<td align=\"center\">推荐在较小卷积层使用DHA，同时加载较大卷积层等待直接执行</td>\n</tr>\n<tr>\n<td align=\"center\">全连接层</td>\n<td align=\"center\">加载快</td>\n<td align=\"center\">执行慢</td>\n<td align=\"center\">推荐在全连接层使用加载后执行，因为其需要频繁访问内存</td>\n</tr>\n<tr>\n<td align=\"center\">嵌入层</td>\n<td align=\"center\">加载较慢</td>\n<td align=\"center\">执行较快</td>\n<td align=\"center\">推荐在嵌入层使用DHA，因为其规模较大，而层中一些参数的访问较少</td>\n</tr>\n<tr>\n<td align=\"center\">归一化层</td>\n<td align=\"center\">LayerNorm更好</td>\n<td align=\"center\">BatchNorm更好</td>\n<td align=\"center\">需要根据具体情况进行选择</td>\n</tr>\n</tbody></table>\n</li>\n</ul>\n<p>产生差异的原因则是不同层对内存访问的需求不同，导致pcie访问次数不同，pcie作为瓶颈，访问次数越多，延迟越大。</p>\n<h3 id=\"并行传输\"><a href=\"#并行传输\" class=\"headerlink\" title=\"并行传输\"></a>并行传输</h3><p>对于多GPU场景，将模型划分为多个部分后，采用并行传输策略：从内存并行地将模型传输到两个GPU，再从第二个GPU向第一个GPU传输，这样可以减少传输时间。<br>在此基础上，可以将GPU2——GPU1的传输变为流水线传输，这样可以进一步减少传输时间。<br>但是，由于CPU提供的PCIE总线数量限制，多GPU系统，例如8GPU也只能每两个GPU公用一组总线，因此多GPU的总线需要考虑总线拓扑。</p>\n<h2 id=\"DeepPlan实现\"><a href=\"#DeepPlan实现\" class=\"headerlink\" title=\"DeepPlan实现\"></a>DeepPlan实现</h2><h3 id=\"整体实现思路\"><a href=\"#整体实现思路\" class=\"headerlink\" title=\"整体实现思路\"></a>整体实现思路</h3><p>再进行训练前，deepPlan会根据每一层的性能分析，推理出当前层采用何种方式进行训练（加载——执行orDHA）。遍历完整个网络后，将根据策略直接执行训练。如果在多GPU系统中，DeepPlan还会根据GPU连连接拓扑，将模型划分为多个部分，应用并行传输方案。</p>\n<h3 id=\"单层性能分析\"><a href=\"#单层性能分析\" class=\"headerlink\" title=\"单层性能分析\"></a>单层性能分析</h3><p>利用单层执行时间的统计数据，或者执行一次单层来得到每一层的性能数据。</p>\n<h3 id=\"层间性能分析\"><a href=\"#层间性能分析\" class=\"headerlink\" title=\"层间性能分析\"></a>层间性能分析</h3><p>对于每层性能已经得到的情况。检查每一层切换策略到DHA后其获得的性能差异是否比加载后执行的停滞时间更短，如果是的话则切换为DHA。并且通过递归的方式检查每个层之前最多可以使用几个DHA来缩短总加载停滞时间。</p>\n<h3 id=\"模型传输规划\"><a href=\"#模型传输规划\" class=\"headerlink\" title=\"模型传输规划\"></a>模型传输规划</h3><p>DeepPlan根据GPU拓扑，和PCIE交换机布局，避免并行加载的总线冲突，检查所选GPU是否使用NVLink，如果使用则直接进行并行传输，否则使用流水线传输。同时，根据并行传输带来的性能优化，重新规划每一层使用的策略。</p>\n",
            "tags": [
                "技术",
                "周报",
                "课题组",
                "神经网络",
                "pytorch"
            ]
        },
        {
            "id": "http://example.com/2023/10/13/%E8%AF%BE%E9%A2%98%E7%BB%84%E7%AC%AC%E5%9B%9B%E5%91%A8%E5%AD%A6%E4%B9%A0/",
            "url": "http://example.com/2023/10/13/%E8%AF%BE%E9%A2%98%E7%BB%84%E7%AC%AC%E5%9B%9B%E5%91%A8%E5%AD%A6%E4%B9%A0/",
            "title": "课题组第四周学习",
            "date_published": "2023-10-13T10:23:34.000Z",
            "content_html": "<h1 id=\"ZeRO-Offload方法\"><a href=\"#ZeRO-Offload方法\" class=\"headerlink\" title=\"ZeRO-Offload方法\"></a>ZeRO-Offload方法</h1><h2 id=\"提出背景\"><a href=\"#提出背景\" class=\"headerlink\" title=\"提出背景\"></a>提出背景</h2><p>对大模型训练来说，GPU显存对参数规模巨大的网络来说是一个瓶颈，然而CPU内存可以做到TB级别，因此可以考虑将一部分参数放在CPU上，而将需要频繁访问的参数放在GPU上，这样可以减少GPU显存的压力，提高训练速度。ZeRO-Offload提出了一种没有数据冗余的优化方法，可以将模型参数分布在CPU和GPU上，而且可以在CPU和GPU之间进行无缝的迁移。</p>\n<h3 id=\"大模型传统方法\"><a href=\"#大模型传统方法\" class=\"headerlink\" title=\"大模型传统方法\"></a>大模型传统方法</h3><p>针对大模型需要的内存过大的问题，传统分为两种方法：</p>\n<ul>\n<li>模型分割：将模型分割成多个部分，每个部分在GPU上训练，然后将结果传递给下一个部分，</li>\n<li>流水线并行：将训练过程分为不同层，每个层分给不同的GPU，然后将结果传递给下一个GPU<h2 id=\"增益来源\"><a href=\"#增益来源\" class=\"headerlink\" title=\"增益来源\"></a>增益来源</h2>根据计算流程，CPU的计算量相比于GPU的$O(MB)$,只有$O(M)$，其中M是模型大小，B是批次大小。<br>这个过程中，ZeRO-Offload将前向与后向传播分配给了GPU，而标准化计算和权重更新等对模型大小有直接联系的计算则分配给了CPU。<br>在数据吞吐方面，cpu与gpu之间仅存在fp16数据的传输，相比与其他方法（例如L2L）有大幅度减少<br>在并行方面，随着计算节点的增加，CPU的计算资源会随着节点数量增加而增加<br>CPU计算通过提高并行性增加了效率</li>\n</ul>\n<h3 id=\"对CPU作为计算瓶颈的解决方法\"><a href=\"#对CPU作为计算瓶颈的解决方法\" class=\"headerlink\" title=\"对CPU作为计算瓶颈的解决方法\"></a>对CPU作为计算瓶颈的解决方法</h3><h4 id=\"对CPU计算的优化\"><a href=\"#对CPU计算的优化\" class=\"headerlink\" title=\"对CPU计算的优化\"></a>对CPU计算的优化</h4><ul>\n<li>向量运算SIMD</li>\n<li>循环展开</li>\n<li>多核并行</li>\n<li>减少缓存抖动<h4 id=\"延迟参数更新\"><a href=\"#延迟参数更新\" class=\"headerlink\" title=\"延迟参数更新\"></a>延迟参数更新</h4>将参数更新延迟，重叠CPU与GPU计算。也就是说，在某一轮计算之后，此后每次gpu使用的优化器参数都是上一轮计算的结果，而不是这一轮计算的结果。，因此可以让cpu计算时间和gpu计算时间重叠。提高流水线负载率。<h2 id=\"优化方法\"><a href=\"#优化方法\" class=\"headerlink\" title=\"优化方法\"></a>优化方法</h2>ZeRO-Offload 同时利用CPU内存计算能力来优化。基于ZeRO优化方法，但是不是像原本多个GPU并行计算，并且通过联系收集器来进行并行。而是把这个通讯过程转化为与CPU的联系，相当于原本多个GPU同时做的工作，让单个GPU进行，每个阶段只进行原先一个GPU进行的工作，同时把其他GPU本应进行的计算状态经由内存进行存储。<h3 id=\"ZeRO的工作\"><a href=\"#ZeRO的工作\" class=\"headerlink\" title=\"ZeRO的工作\"></a>ZeRO的工作</h3>ZeRO，在ZeRO-Offload中使用ZeRO-2阶段，这个阶段你主要是分割模型状态和梯度。在ZeRO-2中，每个GPU都存储着所有参数，但是每轮训练只更新其中不包含的部分。<br>这个过程如下：</li>\n</ul>\n<ol>\n<li>每个GPU进行前馈，计算不同批次的损失。</li>\n<li>每个cpu进行反向传播，并且对每个有梯度的GPU使用减少梯度的算子进行平均。</li>\n<li>反向传播结束后，GPU使用其对应的梯度平均值对其部分参数和优化器状态进行更新。</li>\n<li>进行一次全收集，接收其他GPU计算的参数更新。</li>\n</ol>\n<h3 id=\"ZeRO-Offload的工作\"><a href=\"#ZeRO-Offload的工作\" class=\"headerlink\" title=\"ZeRO-Offload的工作\"></a>ZeRO-Offload的工作</h3><p>ZeRO-Offload将训练修改为数据流图，主要优势：使得CPU计算量减少了几个数量级。保证CPU与GPU通讯最小化。最大限度节省内存。</p>\n<h4 id=\"计算流图\"><a href=\"#计算流图\" class=\"headerlink\" title=\"计算流图\"></a>计算流图</h4><p>计算流图是一种图形化的表示，用于表示计算过程中的数据流动。在计算流图中，节点表示计算，边表示数据流动。<br><img src=\"https://raw.githubusercontent.com/lianga1/picGo_test/main/1.jpg\" alt=\"计算流图\"></p>\n<h4 id=\"减少CPU计算\"><a href=\"#减少CPU计算\" class=\"headerlink\" title=\"减少CPU计算\"></a>减少CPU计算</h4><p>ZeRO-Offload将前向与后向传播分配给了GPU，而标准化计算和权重更新等对模型大小有直接联系的计算则分配给了CPU。</p>\n<h4 id=\"减少CPU与GPU通讯\"><a href=\"#减少CPU与GPU通讯\" class=\"headerlink\" title=\"减少CPU与GPU通讯\"></a>减少CPU与GPU通讯</h4><p>创建fp32区：为了避免fp32数据在pcie总线传输，需要将所有fp32数据放在一个设备上进行处理<br>p16分配：将fp16必须放在前馈与反向传播共同节点的位置，因为这两个节点之间的通信是较大的。<br>因此，ZeRO-Offload将fp16分配给GPU，而将fp32分配给CPU。</p>\n<h4 id=\"减少内存\"><a href=\"#减少内存\" class=\"headerlink\" title=\"减少内存\"></a>减少内存</h4><p>将反向传播后得到的梯度，以及更新梯度所需要的计算和存储空间，写遭到CPU上，可以节省最多的显存使用。</p>\n<h2 id=\"优势\"><a href=\"#优势\" class=\"headerlink\" title=\"优势\"></a>优势</h2><h3 id=\"扩展性强\"><a href=\"#扩展性强\" class=\"headerlink\" title=\"扩展性强\"></a>扩展性强</h3><p>对于任何模型，其优化算法的优化参数对于ZeRO-Offload来说并不关键，其只是需要把fp32的计算内容单独放在CPU中。</p>\n<h3 id=\"支持并行\"><a href=\"#支持并行\" class=\"headerlink\" title=\"支持并行\"></a>支持并行</h3><p>对多个GPU而言。ZeRO-Offload基于ZeRO-2，因此可以将分区的参数分配给多个GPU。</p>\n<h3 id=\"模型并行\"><a href=\"#模型并行\" class=\"headerlink\" title=\"模型并行\"></a>模型并行</h3><p>ZeRO-Offload还可以用模型并行来实现更好的并行性。通过给cpu卸载梯度、优化器状态和优化器计算来和模型并行计算相适应。在这个情况下，首先，借由更难耗尽内存，可以使用更大的批次大小。其次，可以使用更多的GPU来进行模型并行计算。</p>\n",
            "tags": [
                "技术",
                "周报",
                "课题组",
                "神经网络",
                "pytorch"
            ]
        },
        {
            "id": "http://example.com/2023/09/30/%E8%AF%BE%E9%A2%98%E7%BB%84%E7%AC%AC%E4%B8%80%E5%91%A8%E5%AD%A6%E4%B9%A0/",
            "url": "http://example.com/2023/09/30/%E8%AF%BE%E9%A2%98%E7%BB%84%E7%AC%AC%E4%B8%80%E5%91%A8%E5%AD%A6%E4%B9%A0/",
            "title": "课题组第一周学习",
            "date_published": "2023-09-30T11:51:44.000Z",
            "content_html": "<h1 id=\"理论学习\"><a href=\"#理论学习\" class=\"headerlink\" title=\"理论学习\"></a>理论学习</h1><h2 id=\"反向传播算法\"><a href=\"#反向传播算法\" class=\"headerlink\" title=\"反向传播算法\"></a>反向传播算法</h2><p>反向传播是一种基于有监督学习，用于根据误差和损失函数调整网络权重的算法。反向传播算法的核心思想是通过链式法则计算损失函数对于每个权重的梯度，然后使用梯度下降法更新权重。<br>过程：</p>\n<ul>\n<li>首先通过正向传播，根据输入数据得到一个网络的激励</li>\n<li>根据得到的激励与目标值计算损失函数</li>\n<li>根据损失函数，从输出层开始，依次沿着计算图反向计算每个权重的梯度</li>\n<li>根据得到的梯度调整权重<br>[1]\t <a href=\"https://books.google.com/books/about/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8.html?id=2-PWvQEACAAJ\">深度学习入门: 基于Python的理论与实现[M]. 人民邮电出版社, 2018.(p.121,161)</a><h2 id=\"前馈\"><a href=\"#前馈\" class=\"headerlink\" title=\"前馈\"></a>前馈</h2>前馈神经网络是一种最简单的神经网络，它的每个神经元都是前一层神经元的输出。前馈神经网络的每个神经元都是前一层神经元的输出，因此它的输出不会反馈到输入层，这种网络结构也被称为前馈神经网络。</li>\n</ul>\n<h2 id=\"卷积神经网络\"><a href=\"#卷积神经网络\" class=\"headerlink\" title=\"卷积神经网络\"></a>卷积神经网络</h2><h3 id=\"卷积\"><a href=\"#卷积\" class=\"headerlink\" title=\"卷积\"></a>卷积</h3><p>卷积（convolution）是一种数学运算，主要应用于信号处理中对系统响应的计算。卷积运算可以将某个冲激响应针对任意输入进行计算，得到对应的响应结果。卷积运算的公式如下：<br>$$<br>y(t) &#x3D; \\int_{-\\infty}^{\\infty} x(a)h(t-a)da<br>$$<br>其中，$x(t)$为输入信号，$h(t)$为系统响应，$y(t)$为输出信号。</p>\n<h3 id=\"二维离散卷积\"><a href=\"#二维离散卷积\" class=\"headerlink\" title=\"二维离散卷积\"></a>二维离散卷积</h3><p>对于图像处理来说，卷积需要用到二维矩阵的滑动窗口来进行卷积运算。二维离散卷积的公式如下：<br>$$<br>y(i,j) &#x3D; \\sum_{m&#x3D;-\\infty}^{\\infty}\\sum_{n&#x3D;-\\infty}^{\\infty}x(m,n)h(i-m,j-n)<br>$$<br>其中，$x(m,n)$为输入图像，$h(i,j)$为卷积核，$y(i,j)$为输出图像。</p>\n<h3 id=\"卷积神经网络-1\"><a href=\"#卷积神经网络-1\" class=\"headerlink\" title=\"卷积神经网络\"></a>卷积神经网络</h3><p>卷积神经网络（CNN）中，增加了卷积层和池化层。其可以从原本多维度的数据中提取欧氏距离较近的单元之间蕴含的信息。</p>\n<h4 id=\"卷积层\"><a href=\"#卷积层\" class=\"headerlink\" title=\"卷积层\"></a>卷积层</h4><p>在卷积层中，当输入数据是图像时，卷积层会以三维数据形式接收数据，并以三维数据形式传输到下一层，输入输出数据称为特征图（feature map）。其中，卷积层的输入数据称为输入特征图（input feature map），输出数据称为输出特征图（output feature map）。</p>\n<h4 id=\"CNN的处理流\"><a href=\"#CNN的处理流\" class=\"headerlink\" title=\"CNN的处理流\"></a>CNN的处理流</h4><p>针对一个图像，有三维的信息（长、宽、通道），同样，对这个图像进行处理的卷积核也是三维的。但是最终卷积得到的输出结果是二维的（每个通道卷积的结果加在一起）。在CNN中，针对多个卷积核，会得到多个二维的输出结果，这些输出结果会被叠加在一起，得到一个三维的输出结果。这个结果传递给下一层。同时，对多个数据，即批处理，卷积层将多个样本汇总成一次处理，传递中综合成四维的数据。</p>\n<h4 id=\"池化层\"><a href=\"#池化层\" class=\"headerlink\" title=\"池化层\"></a>池化层</h4><p>池化层是一种降低数据大小的方法，它可以减少数据的大小，同时也可以防止过拟合。池化层的处理流程如下：</p>\n<ul>\n<li>按照设定的步长，从输入数据中提取数据块</li>\n<li>例如MAX池化，将数据块中的最大值作为输出结果</li>\n<li>输出结果的规模即随步长变大而缩小<br>同时，池化层输入数据和输出数据的维度相同<h2 id=\"循环神经网络\"><a href=\"#循环神经网络\" class=\"headerlink\" title=\"循环神经网络\"></a>循环神经网络</h2>循环神经网络常用于nlp领域。它和前馈神经网络或CNN的主要区别在于循环神经网络（RNN）的隐藏层的输出不仅仅取决于当前的输入，还取决于前一时刻的隐藏层的输出。因此，RNN具有某种程度上的“记忆”能力。<br>另一个显著特征在于它们在每个网络层共享参数，RNN在每一层都共享相同的参数，这使得它们可以处理任意长度的序列。<br>然而，RNN在反向传播的过程中，梯度会随着时间的推移而消失或爆炸，这使得它们很难学习长期依赖关系。<h2 id=\"注意力机制\"><a href=\"#注意力机制\" class=\"headerlink\" title=\"注意力机制\"></a>注意力机制</h2>注意力机制可以增强神经网络输入数据中某些部分的权重，同时减弱其他部分的权重。<br>例如对一个翻译句子的网络，普通的逐个词翻译会在每一轮翻译过程中对单词序列依次提高注意力，也就是其注意力矩阵会是一个对角线上权值高的矩阵。但是在注意力机制下，每一轮翻译过程中，网络会根据上一轮的翻译结果，对输入句子中的某些部分进行更多的关注，即其权值的最大值不一定在对角线。从而提高翻译的连贯性。<h2 id=\"并行计算\"><a href=\"#并行计算\" class=\"headerlink\" title=\"并行计算\"></a>并行计算</h2>并行计算对计算任务进行拆分，将同时进行的计算任务分配到不同的计算单元上，从而提高计算速度。拆分的方式统称为并行方式，并行计算后的结果重新聚合的方式称为模型更新传递方式。<br>常见的并行方式有：</li>\n<li>数据并行：把数据集切分放到各个计算节点，并在哥哥节点之间传递模型参数</li>\n<li>模型并行：把模型切分放到各个计算节点，并在各个节点之间传递数据。一般把单个算子分配在配置相同的几个硬件上进行模型存储和计算。</li>\n<li>流水线并行：将模型切分成多个阶段，每个阶段在不同的计算节点上进行计算，每个阶段的计算结果传递给下一个阶段。<br>另外，如何更新模型参数也是并行计算的一个重要问题。在硬件组织架构方面，分为参数服务器架构和collective架构。在更新参数方面分为同步和异步更新<a href=\"https://zhuanlan.zhihu.com/p/350501860\">参考内容</a><h3 id=\"allreduce训练\"><a href=\"#allreduce训练\" class=\"headerlink\" title=\"allreduce训练\"></a>allreduce训练</h3>在同步更新参数的训练中，利用AllReduce来整合不同worker的梯度数据。AllReduce有很多种类的实现，主要关注的问题在于不同worker之间传递信息的拓扑结构。例如，对于一个有4个worker的集群，有以下几种拓扑结构：</li>\n<li><strong>ring</strong>：每个worker只和相邻的worker通信</li>\n<li><strong>mesh</strong>：每个worker和所有其他worker通信，但是效率比较低。</li>\n<li><strong>Master-Worker</strong>：一个worker作为master，其他worker作为worker，master和每个worker通信，worker之间不通信。<br>举N个worker的ring结构为例，考察这个结构的工作过程：</li>\n<li>每个worker计算自己的梯度</li>\n<li>每个worker把数据分成N份</li>\n<li>第k个worker把其第k份数据发送给第k+1个worker</li>\n<li>第k个worker把其第k-1份数据和第k-1个worker发送的数据整合，再发给下一个worker</li>\n<li>循环N次之后，每个worker包含最终整合结果的1份</li>\n<li>每个worker把自己的数据发送给下一个worker，收到数据后，每个worker的数据都是最终整合结果<br>这个结构的AllReduce的优势在于发送的数据量是固定的，和worker数量无关，避免了网络拥塞。<a href=\"https://zhuanlan.zhihu.com/p/100012827\">参考内容</a><h1 id=\"实践内容\"><a href=\"#实践内容\" class=\"headerlink\" title=\"实践内容\"></a>实践内容</h1><h2 id=\"lenet5\"><a href=\"#lenet5\" class=\"headerlink\" title=\"lenet5\"></a>lenet5</h2>lenet5是进行手写数字识别的CNN，它的结构如下：<br>输入层-&gt;卷积层-&gt;池化层-&gt;卷积层-&gt;池化层-&gt;全连接层-&gt;全连接层-&gt;输出层（高斯连接）<br>与CNN不同的地方在于，LeNet使用sigmoid函数而非reLU函数。<br>lenet5网络的实现代码如下：<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><div class=\"code-wrapper\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></div></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-class\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title\">LeNet</span>(<span class=\"hljs-params\">nn.Module</span>):</span><br>    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">__init__</span>(<span class=\"hljs-params\">self</span>):</span><br>        <span class=\"hljs-built_in\">super</span>(LeNet, self).__init__()<br>        self.conv = nn.Sequential(<br>            nn.Conv2d(<span class=\"hljs-number\">1</span>, <span class=\"hljs-number\">6</span>, <span class=\"hljs-number\">5</span>), <span class=\"hljs-comment\"># in_channels, out_channels, kernel_size</span><br>            nn.Sigmoid(),<br>            nn.MaxPool2d(<span class=\"hljs-number\">2</span>, <span class=\"hljs-number\">2</span>), <span class=\"hljs-comment\"># kernel_size, stride</span><br>            nn.Conv2d(<span class=\"hljs-number\">6</span>, <span class=\"hljs-number\">16</span>, <span class=\"hljs-number\">5</span>),<br>            nn.Sigmoid(),<br>            nn.MaxPool2d(<span class=\"hljs-number\">2</span>, <span class=\"hljs-number\">2</span>)<br>        )<br>        self.fc = nn.Sequential(<br>            nn.Linear(<span class=\"hljs-number\">16</span>*<span class=\"hljs-number\">4</span>*<span class=\"hljs-number\">4</span>, <span class=\"hljs-number\">120</span>),<br>            nn.Sigmoid(),<br>            nn.Linear(<span class=\"hljs-number\">120</span>, <span class=\"hljs-number\">84</span>),<br>            nn.Sigmoid(),<br>            nn.Linear(<span class=\"hljs-number\">84</span>, <span class=\"hljs-number\">10</span>)<br>        )<br><br>    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">forward</span>(<span class=\"hljs-params\">self, img</span>):</span><br>        feature = self.conv(img)<br>        output = self.fc(feature.view(img.shape[<span class=\"hljs-number\">0</span>], -<span class=\"hljs-number\">1</span>))<br>        <span class=\"hljs-keyword\">return</span> output<br><br></code></pre></td></tr></table></figure>\n这个网络定义了两个部分，一个是卷积层，一个是全连接层。卷积层的输入是一个1通道的图像，输出是一个6通道的图像，卷积核的大小为5*5。全连接层的输入是16*4*4的数据，输出是10个类别的概率。<h2 id=\"resnet\"><a href=\"#resnet\" class=\"headerlink\" title=\"resnet\"></a>resnet</h2>ResNet主要用于解决深度神经网络无法找到更好的解的问题。在深层网络中，梯度消失或爆炸的问题会导致网络无法训练。ResNet通过引入残差块（residual block）来解决这个问题。ResNet将堆叠的几个隐含层作为一个残差块，用残差块拟合的函数从原本的f(x)变为f(x)+x。<br>[4]\t<a href=\"https://arxiv.org/abs/1512.03385\">HE K, ZHANG X, REN S, et al. Deep residual learning for image recognition[C]. Proceedings of the IEEE conference on computer vision and pattern recognition, 2016:770-778.</a><br>通过每个block中残差路径和shortcut路径的设计，可以实现不同的ResNet网络。事实证明，不断增加ResNet的深度，也没有发生解的退化，反而可以提高网络的性能。因此ResNet可以实现如下的网络结构：<br><img src=\"https://raw.githubusercontent.com/lianga1/picGo_test/main/3u8Wwj.png\" alt=\"resnet\"><h3 id=\"实际部署\"><a href=\"#实际部署\" class=\"headerlink\" title=\"实际部署\"></a>实际部署</h3>残差块类定义如下：<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-class\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title\">Bottleneck</span>(<span class=\"hljs-params\">nn.Module</span>):</span><br>    <span class=\"hljs-comment\"># 残差块定义</span><br>    extention = <span class=\"hljs-number\">4</span><br><br>    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">__init__</span>(<span class=\"hljs-params\">self, inplanes, planes, stride, downsample=<span class=\"hljs-literal\">None</span></span>):</span><br>        <span class=\"hljs-built_in\">super</span>(Bottleneck, self).__init__()<br>        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=<span class=\"hljs-number\">1</span>, stride=stride, bias=<span class=\"hljs-literal\">False</span>)<br>        self.bn1 = nn.BatchNorm2d(planes)<br><br>        self.conv2 = nn.Conv2d(planes, planes, kernel_size=<span class=\"hljs-number\">3</span>, stride=<span class=\"hljs-number\">1</span>, padding=<span class=\"hljs-number\">1</span>, bias=<span class=\"hljs-literal\">False</span>)<br>        self.bn2 = nn.BatchNorm2d(planes)<br><br>        self.conv3 = nn.Conv2d(planes, planes * self.extention, kernel_size=<span class=\"hljs-number\">1</span>, stride=<span class=\"hljs-number\">1</span>, bias=<span class=\"hljs-literal\">False</span>)<br>        self.bn3 = nn.BatchNorm2d(planes * self.extention)<br><br>        self.relu = nn.ReLU(inplace=<span class=\"hljs-literal\">True</span>)<br><br>        self.downsample = downsample<br>        self.stride = stride<br></code></pre></td></tr></table></figure>\nResNet网络定义如下：<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-class\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title\">ResNet50</span>(<span class=\"hljs-params\">nn.Module</span>):</span><br>    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">__init__</span>(<span class=\"hljs-params\">self, block, layers, num_class</span>):</span><br>        self.inplane = <span class=\"hljs-number\">64</span><br>        <span class=\"hljs-built_in\">super</span>(ResNet50, self).__init__()<br><br>        self.block = block<br>        self.layers = layers<br><br>        self.conv1 = nn.Conv2d(<span class=\"hljs-number\">3</span>, self.inplane, kernel_size=<span class=\"hljs-number\">7</span>, stride=<span class=\"hljs-number\">2</span>, padding=<span class=\"hljs-number\">3</span>, bias=<span class=\"hljs-literal\">False</span>)<br>        self.bn1 = nn.BatchNorm2d(self.inplane)<br>        self.relu = nn.ReLU()<br>        self.maxpool = nn.MaxPool2d(kernel_size=<span class=\"hljs-number\">3</span>, stride=<span class=\"hljs-number\">2</span>, padding=<span class=\"hljs-number\">1</span>)<br><br>        self.stage1 = self.make_layer(self.block, <span class=\"hljs-number\">64</span>, layers[<span class=\"hljs-number\">0</span>], stride=<span class=\"hljs-number\">1</span>)<br>        self.stage2 = self.make_layer(self.block, <span class=\"hljs-number\">128</span>, layers[<span class=\"hljs-number\">1</span>], stride=<span class=\"hljs-number\">2</span>)<br>        self.stage3 = self.make_layer(self.block, <span class=\"hljs-number\">256</span>, layers[<span class=\"hljs-number\">2</span>], stride=<span class=\"hljs-number\">2</span>)<br>        self.stage4 = self.make_layer(self.block, <span class=\"hljs-number\">512</span>, layers[<span class=\"hljs-number\">3</span>], stride=<span class=\"hljs-number\">2</span>)<br><br>        self.avgpool = nn.AvgPool2d(<span class=\"hljs-number\">7</span>)<br>        self.fc = nn.Linear(<span class=\"hljs-number\">512</span> * block.extention, num_class)<br><br><br></code></pre></td></tr></table></figure>\n在30Epoch后，在测试集的准确度达到了75%。</li>\n</ul>\n<h2 id=\"BERT\"><a href=\"#BERT\" class=\"headerlink\" title=\"BERT\"></a>BERT</h2><p>BERT是基于Transformer的预训练模型，主要用于自然语言处理，它能够预测句子中缺失的词语。以及判断两个句子是不是上下句。<br>整个框架由多层transformer的encoder堆叠而成。encoder由注意力层和feed-forward层组成。<br>BERT中，输入由三种不同embedding组成：</p>\n<ul>\n<li><p>wordpiece embedding：由但词向量组成将单词划分成一组有限公共子词单元。</p>\n</li>\n<li><p>position embedaang：将单词的位置信息编码成特征向量。Transformer通过制定规则来构建一个position embedding</p>\n</li>\n<li><p>segment embedding：用于区分两个句子的向量表示。用于区别问答等非对称子句。</p>\n<h3 id=\"网络结构\"><a href=\"#网络结构\" class=\"headerlink\" title=\"网络结构\"></a>网络结构</h3><p>BERT的主要结构是Transformer，Transformer结构如下图所示：<br><img src=\"https://raw.githubusercontent.com/lianga1/picGo_test/main/20200814234510853.jpg\" alt=\"transformer\"><br>其中左侧部分即为encoder部分。<br>encoder单元由一个multi-head-Attention + Layer Normalization + feedforword + Layer Normalization 叠加产生。<br>在比较大的BERT模型中，有24层encoder，每层有16个Attention，词向量维度1024。在较小情况下，有12层encoder，每层12个Attention，词向量维度768。<br>任何时候feed-forward大小都是词向量维度的4倍。</p>\n<h4 id=\"Attention-Layer\"><a href=\"#Attention-Layer\" class=\"headerlink\" title=\"Attention Layer\"></a>Attention Layer</h4><p>这一层的输入是由X &#x3D; (batch_size,max_len_embedding)构成的。<br>单个self-attention 计算过程是输入X分别和三个矩阵Wq,Wk,Wv相乘，得到Q,K,V。然后计算Q和K的点积，再除以$\\sqrt{d_k}$，再经过softmax函数，得到attention矩阵。最后将attention矩阵和V相乘即加权求和，得到输出。<br>multi-head-Attention将多个不同的self-attention输出进行拼接，然后再乘以一个矩阵W0，得到最终的输出output_sum &#x3D; (batch_size,max_len,n*w_length)这个结果再经过一个全连接层就是整个multi-head-Attention的输出。</p>\n<h4 id=\"Layer-Normalization\"><a href=\"#Layer-Normalization\" class=\"headerlink\" title=\"Layer Normalization\"></a>Layer Normalization</h4><p>这个层相当于对每句话的embedding做归一化，所以用LN而非Batch Normalization</p>\n<h4 id=\"BERT每一层的学习\"><a href=\"#BERT每一层的学习\" class=\"headerlink\" title=\"BERT每一层的学习\"></a>BERT每一层的学习</h4><p>从浅层到深层分别可以学习到surface，短语，语法和语义的信息。</p>\n<h3 id=\"BERT的训练\"><a href=\"#BERT的训练\" class=\"headerlink\" title=\"BERT的训练\"></a>BERT的训练</h3><p>定义几个层的类如下：</p>\n</li>\n<li><p>Embedding：输入的embedding层，包括wordpiece embedding，position embedding，segment embedding</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-class\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title\">Embeddings</span>(<span class=\"hljs-params\">nn.Module</span>):</span><br>    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">__init__</span>(<span class=\"hljs-params\">self</span>):</span><br>        <span class=\"hljs-built_in\">super</span>(Embeddings, self).__init__()<br>        self.seg_emb = nn.Embedding(n_segs, d_model)<br>        self.word_emb = nn.Embedding(max_vocab, d_model)<br>        self.pos_emb = nn.Embedding(max_len, d_model)<br>        self.norm = nn.LayerNorm(d_model)<br>        self.dropout = nn.Dropout(p_dropout)<br><br>    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">forward</span>(<span class=\"hljs-params\">self, x, seg</span>):</span><br>        <span class=\"hljs-string\">&#x27;&#x27;&#x27;</span><br><span class=\"hljs-string\">        x: [batch, seq_len]</span><br><span class=\"hljs-string\">        &#x27;&#x27;&#x27;</span><br>        word_enc = self.word_emb(x)<br><br>        <span class=\"hljs-comment\"># positional embedding</span><br>        pos = torch.arange(x.shape[<span class=\"hljs-number\">1</span>], dtype=torch.long, device=device)<br>        pos = pos.unsqueeze(<span class=\"hljs-number\">0</span>).expand_as(x)<br>        pos_enc = self.pos_emb(pos)<br><br>        seg_enc = self.seg_emb(seg)<br>        x = self.norm(word_enc + pos_enc + seg_enc)<br>        <span class=\"hljs-keyword\">return</span> self.dropout(x)<br>        <span class=\"hljs-comment\"># return: [batch, seq_len, d_model]</span><br></code></pre></td></tr></table></figure>\n</li>\n<li><p>Multi-Head-Attention层</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-class\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title\">ScaledDotProductAttention</span>(<span class=\"hljs-params\">nn.Module</span>):</span><br>    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">__init__</span>(<span class=\"hljs-params\">self</span>):</span><br>        <span class=\"hljs-built_in\">super</span>(ScaledDotProductAttention, self).__init__()<br><br>    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">forward</span>(<span class=\"hljs-params\">self, Q, K, V, attn_mask</span>):</span><br>        scores = torch.matmul(Q, K.transpose(-<span class=\"hljs-number\">1</span>, -<span class=\"hljs-number\">2</span>) / msqrt(d_k))<br>        <span class=\"hljs-comment\"># scores: [batch, n_heads, seq_len, seq_len]</span><br>        scores.masked_fill_(attn_mask, -<span class=\"hljs-number\">1e9</span>)<br>        attn = nn.Softmax(dim=-<span class=\"hljs-number\">1</span>)(scores)<br>        <span class=\"hljs-comment\"># context: [batch, n_heads, seq_len, d_v]</span><br>        context = torch.matmul(attn, V)<br>        <span class=\"hljs-keyword\">return</span> context<br><br><span class=\"hljs-class\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title\">MultiHeadAttention</span>(<span class=\"hljs-params\">nn.Module</span>):</span><br>    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">__init__</span>(<span class=\"hljs-params\">self</span>):</span><br>        <span class=\"hljs-built_in\">super</span>(MultiHeadAttention, self).__init__()<br>        self.W_Q = nn.Linear(d_model, d_k * n_heads, bias=<span class=\"hljs-literal\">False</span>)<br>        self.W_K = nn.Linear(d_model, d_k * n_heads, bias=<span class=\"hljs-literal\">False</span>)<br>        self.W_V = nn.Linear(d_model, d_v * n_heads, bias=<span class=\"hljs-literal\">False</span>)<br>        self.fc = nn.Linear(n_heads * d_v, d_model, bias=<span class=\"hljs-literal\">False</span>)<br><br>    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">forward</span>(<span class=\"hljs-params\">self, Q, K, V, attn_mask</span>):</span><br>        <span class=\"hljs-string\">&#x27;&#x27;&#x27;</span><br><span class=\"hljs-string\">        Q, K, V: [batch, seq_len, d_model]</span><br><span class=\"hljs-string\">        attn_mask: [batch, seq_len, seq_len]</span><br><span class=\"hljs-string\">        &#x27;&#x27;&#x27;</span><br>        batch = Q.size(<span class=\"hljs-number\">0</span>)<br>        <span class=\"hljs-string\">&#x27;&#x27;&#x27;</span><br><span class=\"hljs-string\">        split Q, K, V to per head formula: [batch, seq_len, n_heads, d_k]</span><br><span class=\"hljs-string\">        Convenient for matrix multiply opearation later</span><br><span class=\"hljs-string\">        q, k, v: [batch, n_heads, seq_len, d_k / d_v]</span><br><span class=\"hljs-string\">        &#x27;&#x27;&#x27;</span><br>        per_Q = self.W_Q(Q).view(batch, -<span class=\"hljs-number\">1</span>, n_heads, d_k).transpose(<span class=\"hljs-number\">1</span>, <span class=\"hljs-number\">2</span>)<br>        per_K = self.W_K(K).view(batch, -<span class=\"hljs-number\">1</span>, n_heads, d_k).transpose(<span class=\"hljs-number\">1</span>, <span class=\"hljs-number\">2</span>)<br>        per_V = self.W_V(V).view(batch, -<span class=\"hljs-number\">1</span>, n_heads, d_v).transpose(<span class=\"hljs-number\">1</span>, <span class=\"hljs-number\">2</span>)<br><br>        attn_mask = attn_mask.unsqueeze(<span class=\"hljs-number\">1</span>).repeat(<span class=\"hljs-number\">1</span>, n_heads, <span class=\"hljs-number\">1</span>, <span class=\"hljs-number\">1</span>)<br>        <span class=\"hljs-comment\"># context: [batch, n_heads, seq_len, d_v]</span><br>        context = ScaledDotProductAttention()(per_Q, per_K, per_V, attn_mask)<br>        context = context.transpose(<span class=\"hljs-number\">1</span>, <span class=\"hljs-number\">2</span>).contiguous().view(<br>            batch, -<span class=\"hljs-number\">1</span>, n_heads * d_v)<br><br>        <span class=\"hljs-comment\"># output: [batch, seq_len, d_model]</span><br>        output = self.fc(context)<br>        <span class=\"hljs-keyword\">return</span> output<br></code></pre></td></tr></table></figure></li>\n<li><p>其余层，包括FeedForword层和池化层</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-class\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title\">FeedForwardNetwork</span>(<span class=\"hljs-params\">nn.Module</span>):</span><br>    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">__init__</span>(<span class=\"hljs-params\">self</span>):</span><br>        <span class=\"hljs-built_in\">super</span>(FeedForwardNetwork, self).__init__()<br>        self.fc1 = nn.Linear(d_model, d_ff)<br>        self.fc2 = nn.Linear(d_ff, d_model)<br>        self.dropout = nn.Dropout(p_dropout)<br>        self.gelu = gelu<br><br>    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">forward</span>(<span class=\"hljs-params\">self, x</span>):</span><br>        x = self.fc1(x)<br>        x = self.dropout(x)<br>        x = self.gelu(x)<br>        x = self.fc2(x)<br>        <span class=\"hljs-keyword\">return</span> x<br><br><span class=\"hljs-class\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title\">Pooler</span>(<span class=\"hljs-params\">nn.Module</span>):</span><br>    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">__init__</span>(<span class=\"hljs-params\">self</span>):</span><br>        <span class=\"hljs-built_in\">super</span>(Pooler, self).__init__()<br>        self.fc = nn.Linear(d_model, d_model)<br>        self.tanh = nn.Tanh()<br><br>    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">forward</span>(<span class=\"hljs-params\">self, x</span>):</span><br>        <span class=\"hljs-string\">&#x27;&#x27;&#x27;</span><br><span class=\"hljs-string\">        x: [batch, d_model] (first place output)</span><br><span class=\"hljs-string\">        &#x27;&#x27;&#x27;</span><br>        x = self.fc(x)<br>        x = self.tanh(x)<br>        <span class=\"hljs-keyword\">return</span> x<br><br></code></pre></td></tr></table></figure></li>\n<li><p>Encoder层和组合而成的BERT网络</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-class\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title\">EncoderLayer</span>(<span class=\"hljs-params\">nn.Module</span>):</span><br>    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">__init__</span>(<span class=\"hljs-params\">self</span>):</span><br>        <span class=\"hljs-built_in\">super</span>(EncoderLayer, self).__init__()<br>        self.norm1 = nn.LayerNorm(d_model)<br>        self.norm2 = nn.LayerNorm(d_model)<br><br>        self.enc_attn = MultiHeadAttention()<br>        self.ffn = FeedForwardNetwork()<br><br>    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">forward</span>(<span class=\"hljs-params\">self, x, pad_mask</span>):</span><br>        <span class=\"hljs-string\">&#x27;&#x27;&#x27;</span><br><span class=\"hljs-string\">        pre-norm</span><br><span class=\"hljs-string\">        see more detail in https://openreview.net/pdf?id=B1x8anVFPr</span><br><span class=\"hljs-string\"></span><br><span class=\"hljs-string\">        x: [batch, seq_len, d_model]</span><br><span class=\"hljs-string\">        &#x27;&#x27;&#x27;</span><br>        residual = x<br>        x = self.norm1(x)<br>        x = self.enc_attn(x, x, x, pad_mask) + residual<br>        residual = x<br>        x = self.norm2(x)<br>        x = self.ffn(x)<br>        <span class=\"hljs-keyword\">return</span> x + residual<br><br><br><span class=\"hljs-class\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title\">BERT</span>(<span class=\"hljs-params\">nn.Module</span>):</span><br>    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">__init__</span>(<span class=\"hljs-params\">self, n_layers</span>):</span><br>        <span class=\"hljs-built_in\">super</span>(BERT, self).__init__()<br>        self.embedding = Embeddings()<br>        self.encoders = nn.ModuleList([<br>            EncoderLayer() <span class=\"hljs-keyword\">for</span> _ <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">range</span>(n_layers)<br>        ])<br><br>        self.pooler = Pooler()<br><br>        self.next_cls = nn.Linear(d_model, <span class=\"hljs-number\">2</span>)<br>        self.gelu = gelu<br><br>        shared_weight = self.pooler.fc.weight<br>        self.fc = nn.Linear(d_model, d_model)<br>        self.fc.weight = shared_weight<br><br>        shared_weight = self.embedding.word_emb.weight<br>        self.word_classifier = nn.Linear(d_model, max_vocab, bias=<span class=\"hljs-literal\">False</span>)<br>        self.word_classifier.weight = shared_weight<br><br>    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">forward</span>(<span class=\"hljs-params\">self, tokens, segments, masked_pos</span>):</span><br>        output = self.embedding(tokens, segments)<br>        enc_self_pad_mask = get_pad_mask(tokens)<br>        <span class=\"hljs-keyword\">for</span> layer <span class=\"hljs-keyword\">in</span> self.encoders:<br>            output = layer(output, enc_self_pad_mask)<br>        <span class=\"hljs-comment\"># output: [batch, max_len, d_model]</span><br><br>        <span class=\"hljs-comment\"># NSP Task</span><br>        hidden_pool = self.pooler(output[:, <span class=\"hljs-number\">0</span>])<br>        logits_cls = self.next_cls(hidden_pool)<br><br>        <span class=\"hljs-comment\"># Masked Language Model Task</span><br>        <span class=\"hljs-comment\"># masked_pos: [batch, max_pred] -&gt; [batch, max_pred, d_model]</span><br>        masked_pos = masked_pos.unsqueeze(-<span class=\"hljs-number\">1</span>).expand(-<span class=\"hljs-number\">1</span>, -<span class=\"hljs-number\">1</span>, d_model)<br><br>        <span class=\"hljs-comment\"># h_masked: [batch, max_pred, d_model]</span><br>        h_masked = torch.gather(output, dim=<span class=\"hljs-number\">1</span>, index=masked_pos)<br>        h_masked = self.gelu(self.fc(h_masked))<br>        logits_lm = self.word_classifier(h_masked)<br>        <span class=\"hljs-comment\"># logits_lm: [batch, max_pred, max_vocab]</span><br>        <span class=\"hljs-comment\"># logits_cls: [batch, 2]</span><br><br>        <span class=\"hljs-keyword\">return</span> logits_cls, logits_lm<br><br></code></pre></td></tr></table></figure>\n<p>batch-size设为6<br>训练300个Epoch<br>训练结果进行预测例句<br>结果如下：</p>\n<figure class=\"highlight routeros\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs routeros\">========================================================<br>Masked data:<br>[<span class=\"hljs-string\">&#x27;[CLS]&#x27;</span>, <span class=\"hljs-string\">&#x27;[MASK]&#x27;</span>, <span class=\"hljs-string\">&#x27;[MASK]&#x27;</span>, <span class=\"hljs-string\">&#x27;you&#x27;</span>, <span class=\"hljs-string\">&#x27;too&#x27;</span>, <span class=\"hljs-string\">&#x27;how&#x27;</span>, <span class=\"hljs-string\">&#x27;are&#x27;</span>, <span class=\"hljs-string\">&#x27;you&#x27;</span>, <span class=\"hljs-string\">&#x27;today&#x27;</span>, <span class=\"hljs-string\">&#x27;[SEP]&#x27;</span>, <span class=\"hljs-string\">&#x27;great&#x27;</span>,<br> <span class=\"hljs-string\">&#x27;my&#x27;</span>, <span class=\"hljs-string\">&#x27;baseball&#x27;</span>, <span class=\"hljs-string\">&#x27;team&#x27;</span>, <span class=\"hljs-string\">&#x27;won&#x27;</span>, <span class=\"hljs-string\">&#x27;the&#x27;</span>, <span class=\"hljs-string\">&#x27;competition&#x27;</span>, <span class=\"hljs-string\">&#x27;[SEP]&#x27;</span>]<br>BERT reconstructed:<br>[<span class=\"hljs-string\">&#x27;[CLS]&#x27;</span>, <span class=\"hljs-string\">&#x27;nice&#x27;</span>, <span class=\"hljs-string\">&#x27;meet&#x27;</span>, <span class=\"hljs-string\">&#x27;you&#x27;</span>, <span class=\"hljs-string\">&#x27;too&#x27;</span>, <span class=\"hljs-string\">&#x27;how&#x27;</span>, <span class=\"hljs-string\">&#x27;are&#x27;</span>, <span class=\"hljs-string\">&#x27;you&#x27;</span>, <span class=\"hljs-string\">&#x27;today&#x27;</span>, <span class=\"hljs-string\">&#x27;[SEP]&#x27;</span>, <span class=\"hljs-string\">&#x27;great&#x27;</span>,<br> <span class=\"hljs-string\">&#x27;my&#x27;</span>, <span class=\"hljs-string\">&#x27;baseball&#x27;</span>, <span class=\"hljs-string\">&#x27;team&#x27;</span>, <span class=\"hljs-string\">&#x27;won&#x27;</span>, <span class=\"hljs-string\">&#x27;the&#x27;</span>, <span class=\"hljs-string\">&#x27;competition&#x27;</span>, <span class=\"hljs-string\">&#x27;[SEP]&#x27;</span>]<br>Original sentence:<br>[<span class=\"hljs-string\">&#x27;[CLS]&#x27;</span>, <span class=\"hljs-string\">&#x27;nice&#x27;</span>, <span class=\"hljs-string\">&#x27;meet&#x27;</span>, <span class=\"hljs-string\">&#x27;you&#x27;</span>, <span class=\"hljs-string\">&#x27;too&#x27;</span>, <span class=\"hljs-string\">&#x27;how&#x27;</span>, <span class=\"hljs-string\">&#x27;are&#x27;</span>, <span class=\"hljs-string\">&#x27;you&#x27;</span>, <span class=\"hljs-string\">&#x27;today&#x27;</span>, <span class=\"hljs-string\">&#x27;[SEP]&#x27;</span>, <span class=\"hljs-string\">&#x27;great&#x27;</span>,<br> <span class=\"hljs-string\">&#x27;my&#x27;</span>, <span class=\"hljs-string\">&#x27;baseball&#x27;</span>, <span class=\"hljs-string\">&#x27;team&#x27;</span>, <span class=\"hljs-string\">&#x27;won&#x27;</span>, <span class=\"hljs-string\">&#x27;the&#x27;</span>, <span class=\"hljs-string\">&#x27;competition&#x27;</span>, <span class=\"hljs-string\">&#x27;[SEP]&#x27;</span>]<br>===============Next Sentence <span class=\"hljs-attribute\">Prediction</span>===============<br>Two sentences are continuous? <span class=\"hljs-literal\">True</span><br>BERT predict: <span class=\"hljs-literal\">True</span><br></code></pre></td></tr></table></figure></li>\n</ul>\n",
            "tags": [
                "技术",
                "周报",
                "课题组",
                "神经网络",
                "pytorch"
            ]
        },
        {
            "id": "http://example.com/2023/09/24/%E5%85%B3%E4%BA%8Eselenium%E5%8C%85%E5%AE%89%E8%A3%85%E8%BF%90%E8%A1%8C%E7%9A%84%E9%97%AE%E9%A2%98%E6%8E%92%E9%99%A4/",
            "url": "http://example.com/2023/09/24/%E5%85%B3%E4%BA%8Eselenium%E5%8C%85%E5%AE%89%E8%A3%85%E8%BF%90%E8%A1%8C%E7%9A%84%E9%97%AE%E9%A2%98%E6%8E%92%E9%99%A4/",
            "title": "关于selenium包安装运行的问题排除",
            "date_published": "2023-09-24T09:03:44.000Z",
            "content_html": "<h1 id=\"selenium-包安装\"><a href=\"#selenium-包安装\" class=\"headerlink\" title=\"selenium 包安装\"></a>selenium 包安装</h1><p>首先是想要在base环境下安装的，但是因为base环境的内容太多，solve解决依赖问题耗时过长，所以考虑新建环境。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><div class=\"code-wrapper\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></div></td><td class=\"code\"><pre><code class=\"hljs bash\">conda create -n webdriver python=3.7<br>conda activate webdriver<br>conda install selenium<br></code></pre></td></tr></table></figure>\n<h1 id=\"出现问题\"><a href=\"#出现问题\" class=\"headerlink\" title=\"出现问题\"></a>出现问题</h1><p>在按照例程运行代码时，出现了以下问题：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-keyword\">from</span> selenium <span class=\"hljs-keyword\">import</span> webdriver<br><span class=\"hljs-keyword\">import</span> pandas <span class=\"hljs-keyword\">as</span> pd<br><span class=\"hljs-keyword\">import</span> platform<br><span class=\"hljs-keyword\">import</span> matplotlib.pyplot <span class=\"hljs-keyword\">as</span> plt<br><span class=\"hljs-keyword\">import</span> os<br>chromedriver = os.path.abspath(<span class=\"hljs-string\">&#x27;C:\\\\Users\\\\18381\\\\anaconda3\\\\Scripts\\\\chromedriver.exe&#x27;</span>)<br>os.environ[<span class=\"hljs-string\">&quot;webdriver.chrome.driver&quot;</span>] = chromedriver<br>driver = webdriver.Chrome()<br></code></pre></td></tr></table></figure>\n\n<p>出现了以下错误：<br>Unable to obtain driver using Selenium Manager: C:\\Users\\18381\\anaconda3\\envs\\webdriver\\lib\\site-packages\\selenium\\webdriver\\common\\windows\\selenium-manager.exe is missing.</p>\n<p>因此，查阅github上有关issue，发现是conda打包问题，没有打包这个可执行文件。因此，需要手动下载这个文件，放到对应的目录下。<br><a href=\"https://github.com/SeleniumHQ/selenium/tree/trunk/common/manager\">下载文件</a></p>\n<h1 id=\"其他需要注意的地方\"><a href=\"#其他需要注意的地方\" class=\"headerlink\" title=\"其他需要注意的地方\"></a>其他需要注意的地方</h1><p>比如求解器，可以使用新的求解器例如<a href=\"https://www.anaconda.com/blog/a-faster-conda-for-a-growing-community\">libmamba</a><br>虽然我还没完全搞懂这个东西如何使用</p>\n<p>需要先安装chrome和对应的chromedriver<br><a href=\"https://chromedriver.chromium.org/downloads\">chromedriver</a></p>\n",
            "tags": [
                "技术",
                "python"
            ]
        },
        {
            "id": "http://example.com/2023/07/05/%E7%94%B5%E8%B5%9B%E5%9F%B9%E8%AE%AD-23-07-05/",
            "url": "http://example.com/2023/07/05/%E7%94%B5%E8%B5%9B%E5%9F%B9%E8%AE%AD-23-07-05/",
            "title": "电赛培训-23-07-05",
            "date_published": "2023-07-05T01:46:48.000Z",
            "content_html": "<h1 id=\"Arduino\"><a href=\"#Arduino\" class=\"headerlink\" title=\"Arduino\"></a>Arduino</h1><p>软件：使用Arduino IDE，以C++风格语言编写相关库。<br>利用IDE编译固件下载到Arduino的前提条件是有bootloader程序。<br>所以从程序到固件的关键在于bootloader，其他单片机也可以装载bootloader固件，然后使用arduino库。</p>\n<h2 id=\"硬件-uno板\"><a href=\"#硬件-uno板\" class=\"headerlink\" title=\"硬件 uno板\"></a>硬件 uno板</h2><p>工作电压5v,可以typeB，DC5.5，或者跳线接入<br>共有14个数字输入输出（6个PWM口），6个模拟输入输出</p>\n<h2 id=\"系统指示灯\"><a href=\"#系统指示灯\" class=\"headerlink\" title=\"系统指示灯\"></a>系统指示灯</h2><ul>\n<li>ON：系统指示灯</li>\n<li>RX：接收指示灯</li>\n<li>TX：发送指示灯</li>\n<li>L：内置LED，对应13号数字口<h2 id=\"开始编写代码\"><a href=\"#开始编写代码\" class=\"headerlink\" title=\"开始编写代码\"></a>开始编写代码</h2>有两个一定会有的函数，void setup()和void loop()，分别是初始化和循环函数。<br>setup会执行一次，loop在setup后会自动循环<h3 id=\"setup函数\"><a href=\"#setup函数\" class=\"headerlink\" title=\"setup函数\"></a>setup函数</h3>setup中用pinMode配置管脚模式为输出<br>pinMode(pin编号，INPUT)：高阻态，可认为是100m欧姆，电平不定。<br>pinMode(pinnum,INPUT_PULLUP):内置上拉输入，无外部信号默认高电平。<br>pinMode(pinnum,OUTPUT):输出模式，uno上高电平5v，电流&lt;40mA</li>\n</ul>\n<h3 id=\"loop函数\"><a href=\"#loop函数\" class=\"headerlink\" title=\"loop函数\"></a>loop函数</h3><ul>\n<li>digitalWrite(pinnum,HIGH&#x2F;LOW):输出高低电平，只对output模式有效</li>\n<li>digitalRead(pinnum):读取高低电平,返回HIGH&#x2F;LOW两种电平</li>\n<li>analogRead(anaPinNum)：读取模拟输入电平，返回0-1023的数字，对应0-5v的电压</li>\n<li>analogWrite(pwmPinNum,0-255)：输出PWM波，对应0-5v的电压，频率为490Hz(3,9,10,11pin),或980Hz(5,6pin)</li>\n<li>analogReference(AD参考电压输入来源)：切换AD参考电压输入来源，有默认值，一般不用，可以让输出更加精细。</li>\n</ul>\n<h3 id=\"中断和轮询\"><a href=\"#中断和轮询\" class=\"headerlink\" title=\"中断和轮询\"></a>中断和轮询</h3><p>轮询：不断重复读取某个状态值，缺点是占用资源<br>中断：可以通过某个状态改变来发送信号，然后发送信号后可以执行其他操作，之后再恢复到发送信号之前的状态。<br>管脚中断：attachInterrupt(digitalPinToInterrupt(pinnum),ISR,mode),第一个参数是中断管脚号（uno为2，3），第二个参数是中断服务函数（可以自定义），第三个参数是中断模式，有LOW，RISING，FALLING，CHANGE四种模式。</p>\n<ul>\n<li>LOW：低电平触发</li>\n<li>RISING：上升沿触发</li>\n<li>FALLING：下降沿触发</li>\n<li>CHANGE：任意电平变化触发</li>\n</ul>\n<p><strong>注意，终端服务函数应当很短，而且不能使用其他中断实现的函数，延时需要delayMicroseconds(us)</strong><br><strong>修改全局变量应当用volatile修饰，防止编译器优化</strong></p>\n<h1 id=\"ESP32\"><a href=\"#ESP32\" class=\"headerlink\" title=\"ESP32\"></a>ESP32</h1><h2 id=\"硬件\"><a href=\"#硬件\" class=\"headerlink\" title=\"硬件\"></a>硬件</h2><p>esp32-WROOM-32<br>串口芯片：CP2102<br>核心频率240mHz<br>WiFi IEEE 802.11 b&#x2F;g&#x2F;n 2.4GHz<br>BLuetooth 4.2 BR&#x2F;EDR and BLE<br>520k SRAM 448kB ROM<br>2个I2S，RMT远程控制，LED PWM，1个host SD&#x2F;eMMC&#x2F;SDIO，一个slave SDIO&#x2F;SPI. TWAI(CAN),12bitADC,Ethernet</p>\n<h2 id=\"开发环境\"><a href=\"#开发环境\" class=\"headerlink\" title=\"开发环境\"></a>开发环境</h2><p>MicroPython+Thonny</p>\n<h3 id=\"常用库\"><a href=\"#常用库\" class=\"headerlink\" title=\"常用库\"></a>常用库</h3><h3 id=\"GPIO\"><a href=\"#GPIO\" class=\"headerlink\" title=\"GPIO\"></a>GPIO</h3><figure class=\"highlight pgsql\"><table><tr><td class=\"gutter\"><div class=\"code-wrapper\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></div></td><td class=\"code\"><pre><code class=\"hljs pgsql\"><span class=\"hljs-keyword\">from</span> machine <span class=\"hljs-keyword\">import</span> Pin<br><br>p0 = Pin(<span class=\"hljs-number\">0</span>,Pin.<span class=\"hljs-keyword\">OUT</span>) # <span class=\"hljs-keyword\">create</span> output pin <span class=\"hljs-keyword\">on</span> GPIO0<br>p0.<span class=\"hljs-keyword\">on</span>() # <span class=\"hljs-keyword\">set</span> pin <span class=\"hljs-keyword\">to</span> &quot;on&quot; (high) <span class=\"hljs-keyword\">level</span><br>p0.<span class=\"hljs-keyword\">off</span>() # <span class=\"hljs-keyword\">set</span> pin <span class=\"hljs-keyword\">to</span> &quot;off&quot; (low) <span class=\"hljs-keyword\">level</span><br>p0.<span class=\"hljs-keyword\">value</span>(<span class=\"hljs-number\">1</span>) # <span class=\"hljs-keyword\">set</span> pin <span class=\"hljs-keyword\">to</span> <span class=\"hljs-keyword\">on</span>/high<br>p0.init(p0.<span class=\"hljs-keyword\">IN</span>,p0.PULL_DOWN) # <span class=\"hljs-keyword\">set</span> pin <span class=\"hljs-keyword\">to</span> <span class=\"hljs-keyword\">input</span> <span class=\"hljs-keyword\">with</span> a pull-down resistor<br></code></pre></td></tr></table></figure>\n<p>init函数中，id是强制的</p>\n<ul>\n<li><p>mode指定引脚模式，有IN，OUT，OPEN_DRAIN，AF_OPEN_DRAIN四种模式</p>\n</li>\n<li><p>pull指定引脚是否连接弱上拉电阻，有None，PULL_UP，PULL_DOWN三种模式<br>弱上拉指上拉电阻阻值较大，高电平很容易因为外部电流驱动而拉低。</p>\n</li>\n<li><p>drive具有不同的最大安全电流的限制，有DRIVE_0-3四种选择</p>\n</li>\n<li><p>alt为引脚的备用功能，仅对alt和alt_open_drain两种模式有效，有0-7八种选择</p>\n</li>\n</ul>\n<p>value函数中，如果不带参数，就是得到当前状态，如果在输出模式，需要带参数，变为设置电平</p>\n<p>配置在引脚的触发源处于活动状态时要调用中断处理程序，如果引脚模式为Pin.IN，可以使用irq函数，如果引脚模式为Pin.IN，可以使用Pin.IRQ_RISING，Pin.IRQ_FALLING，Pin.IRQ_ANY三种模式，分别对应上升沿，下降沿，任意电平变化触发中断。</p>\n",
            "tags": [
                "技术",
                "博客",
                "电赛"
            ]
        },
        {
            "id": "http://example.com/2023/05/29/%E5%85%B3%E4%BA%8E%E8%A7%A3%E5%86%B3WSL-Ubuntu%E6%89%BE%E4%B8%8D%E5%88%B0sys-time-h%E7%9A%84%E9%97%AE%E9%A2%98/",
            "url": "http://example.com/2023/05/29/%E5%85%B3%E4%BA%8E%E8%A7%A3%E5%86%B3WSL-Ubuntu%E6%89%BE%E4%B8%8D%E5%88%B0sys-time-h%E7%9A%84%E9%97%AE%E9%A2%98/",
            "title": "关于解决WSL_Ubuntu找不到sys/time.h的问题",
            "date_published": "2023-05-29T07:24:24.000Z",
            "content_html": "<h1 id=\"问题描述\"><a href=\"#问题描述\" class=\"headerlink\" title=\"问题描述\"></a>问题描述</h1><p>在使用WSL_Ubuntu的时候，编译C代码时，出现了找不到sys&#x2F;time.h的问题</p>\n<h1 id=\"解决方法\"><a href=\"#解决方法\" class=\"headerlink\" title=\"解决方法\"></a>解决方法</h1><h2 id=\"第一次失败\"><a href=\"#第一次失败\" class=\"headerlink\" title=\"第一次失败\"></a>第一次失败</h2><p>网上的一个直接解决方案是安装libc6-dev-amd64<br>但是问题又一次出现，当输入指令<br><code>sudo apt-get install libc6-dev-amd64</code><br>发生报错 unable to locate package</p>\n<h2 id=\"第二次失败\"><a href=\"#第二次失败\" class=\"headerlink\" title=\"第二次失败\"></a>第二次失败</h2><p>于是转而解决无法定位包的问题，根据查找发现需要在&#x2F;etc&#x2F;apt&#x2F;sources.list中添加源,添加了清华源、阿里源后输入<br><code>sudo apt-get update</code><br>更新完成后再次尝试安装libc6-dev-amd64，但是问题依旧存在</p>\n<h2 id=\"第三次解决\"><a href=\"#第三次解决\" class=\"headerlink\" title=\"第三次解决\"></a>第三次解决</h2><p>这次发现libc6-dev-amd64是一个需要在i386架构下安装的包，于是尝试添加i386架构，运行指令<br><code>dpkg --add-architecture i386</code><br>添加成功后再次输入<br><code>sudo apt-get update</code><br>更新完成后再次尝试安装libc6-dev-amd64，问题解决</p>\n",
            "tags": [
                "技术",
                "博客",
                "WSL",
                "Linux",
                "编译"
            ]
        },
        {
            "id": "http://example.com/2023/05/14/%E7%94%B5%E8%B5%9B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89/",
            "url": "http://example.com/2023/05/14/%E7%94%B5%E8%B5%9B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89/",
            "title": "电赛学习笔记-机器视觉",
            "date_published": "2023-05-14T09:04:31.000Z",
            "content_html": "<h1 id=\"开发环境\"><a href=\"#开发环境\" class=\"headerlink\" title=\"开发环境\"></a>开发环境</h1><h2 id=\"安装opencv\"><a href=\"#安装opencv\" class=\"headerlink\" title=\"安装opencv\"></a>安装opencv</h2><h2 id=\"开发板：STM32F407\"><a href=\"#开发板：STM32F407\" class=\"headerlink\" title=\"开发板：STM32F407\"></a>开发板：STM32F407</h2><h2 id=\"IDE：STM32CubeIDE\"><a href=\"#IDE：STM32CubeIDE\" class=\"headerlink\" title=\"IDE：STM32CubeIDE\"></a>IDE：STM32CubeIDE</h2><h2 id=\"配置过程\"><a href=\"#配置过程\" class=\"headerlink\" title=\"配置过程\"></a>配置过程</h2><p>配置工程ioc文件，配置好基础外设后，再packs中安装X-CUBE-AI组件包，在软件包外设中添加模型文件，设置压缩倍数，导入测试集验证准确率</p>\n<h2 id=\"模型\"><a href=\"#模型\" class=\"headerlink\" title=\"模型\"></a>模型</h2><h3 id=\"yolo\"><a href=\"#yolo\" class=\"headerlink\" title=\"yolo\"></a>yolo</h3><p>利用mobilenet yolo50k模型可以导入到单片机中，只需要较少内存即可实现实时运行，实现人脸识别的功能</p>\n<h3 id=\"openmv\"><a href=\"#openmv\" class=\"headerlink\" title=\"openmv\"></a>openmv</h3><h2 id=\"硬件\"><a href=\"#硬件\" class=\"headerlink\" title=\"硬件\"></a>硬件</h2><p>正点原子ov7725摄像头<br>yolo50k</p>\n<h1 id=\"相关资料\"><a href=\"#相关资料\" class=\"headerlink\" title=\"相关资料\"></a>相关资料</h1><p><a href=\"https://www.bilibili.com/video/BV1Bt411w77m/?share_source=copy_web&vd_source=4ed5c2c0429d7681216f506ac1e74065\">稚晖君</a><br><a href=\"https://github.com/dog-qiuqiu/MobileNet-Yolo\">yolo50k仓库</a><br><a href=\"https://www.bilibili.com/video/BV1FL411u72p/?share_source=copy_web&vd_source=4ed5c2c0429d7681216f506ac1e74065\">实时运行案例</a></p>\n",
            "tags": [
                "技术",
                "博客",
                "电赛"
            ]
        },
        {
            "id": "http://example.com/2023/05/14/%E7%94%B5%E8%B5%9B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-micropython/",
            "url": "http://example.com/2023/05/14/%E7%94%B5%E8%B5%9B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-micropython/",
            "title": "电赛学习笔记-micropython",
            "date_published": "2023-05-14T08:47:54.000Z",
            "content_html": "<h1 id=\"micropython简介\"><a href=\"#micropython简介\" class=\"headerlink\" title=\"micropython简介\"></a>micropython简介</h1><p>micropython是一个能够利用python进行单片机开发的固件，目前主要是在esp32平台上进行的开发</p>\n<h1 id=\"micropython安装\"><a href=\"#micropython安装\" class=\"headerlink\" title=\"micropython安装\"></a>micropython安装</h1><ul>\n<li>在micropython官网找到对应的单片机的型号的固件文件（.bin），下载到对应位置</li>\n<li>pip install esptool</li>\n<li>连接esp32单片机，查看端口号</li>\n<li>根据micropython官网的指示，利用esptool.py文件，清除单片机flash，再部署固件到单片机。</li>\n<li>安装uPyCraft IDE，选择好开发板类型和端口号后，<h1 id=\"micropython使用\"><a href=\"#micropython使用\" class=\"headerlink\" title=\"micropython使用\"></a>micropython使用</h1>需要根据单片机自带的库函数，进行python文档的开发<h1 id=\"micropython的优点\"><a href=\"#micropython的优点\" class=\"headerlink\" title=\"micropython的优点\"></a>micropython的优点</h1>代码量少，配置简单</li>\n</ul>\n",
            "tags": [
                "技术",
                "博客",
                "电赛"
            ]
        },
        {
            "id": "http://example.com/2023/05/11/%E7%94%B5%E8%B5%9B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%881%EF%BC%89/",
            "url": "http://example.com/2023/05/11/%E7%94%B5%E8%B5%9B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%881%EF%BC%89/",
            "title": "电赛学习笔记（1）——stm32学习笔记",
            "date_published": "2023-05-11T08:19:20.000Z",
            "content_html": "<h1 id=\"stm32基础\"><a href=\"#stm32基础\" class=\"headerlink\" title=\"stm32基础\"></a>stm32基础</h1><h2 id=\"关于stm32产品线\"><a href=\"#关于stm32产品线\" class=\"headerlink\" title=\"关于stm32产品线\"></a>关于stm32产品线</h2><h3 id=\"stm32f1系列-cortex-m3\"><a href=\"#stm32f1系列-cortex-m3\" class=\"headerlink\" title=\"stm32f1系列 cortex-m3\"></a>stm32f1系列 cortex-m3</h3><h3 id=\"stm32f4系列-cortex-m4\"><a href=\"#stm32f4系列-cortex-m4\" class=\"headerlink\" title=\"stm32f4系列 cortex-m4\"></a>stm32f4系列 cortex-m4</h3><ul>\n<li>内置了rgb lcd驱动</li>\n<li>加入了DSP与FPU模块<h3 id=\"stm32f7系列-cortex-m7\"><a href=\"#stm32f7系列-cortex-m7\" class=\"headerlink\" title=\"stm32f7系列 cortex-m7\"></a>stm32f7系列 cortex-m7</h3></li>\n<li>高速内存得到应用 <h2 id=\"寄存器编程\"><a href=\"#寄存器编程\" class=\"headerlink\" title=\"寄存器编程\"></a>寄存器编程</h2></li>\n</ul>\n<p><strong>关键字volatile</strong>需要在声明寄存器变量的时候添加，因为要防止编译器自行优化。</p>\n<h2 id=\"HAL库\"><a href=\"#HAL库\" class=\"headerlink\" title=\"HAL库\"></a>HAL库</h2><p>硬件抽象层，可以将不同产品线的芯片的寄存器操作抽象为函数，方便移植<br><strong><font color=\"red\">本笔记使用HAL库进行编程</font></strong><br>相对的，HAL库会产生大量的判断来降低代码运行效率<br>但是，还有另一个Low Layer库（LL），这个库可以提高效率</p>\n<h2 id=\"stm32cubeMX配置\"><a href=\"#stm32cubeMX配置\" class=\"headerlink\" title=\"stm32cubeMX配置\"></a>stm32cubeMX配置</h2><h2 id=\"stm32计时器\"><a href=\"#stm32计时器\" class=\"headerlink\" title=\"stm32计时器\"></a>stm32计时器</h2><h3 id=\"PWM调制输出\"><a href=\"#PWM调制输出\" class=\"headerlink\" title=\"PWM调制输出\"></a>PWM调制输出</h3><h4 id=\"几个重要参数\"><a href=\"#几个重要参数\" class=\"headerlink\" title=\"几个重要参数\"></a>几个重要参数</h4><ul>\n<li>占空比：高电平占整个周期的比例</li>\n<li>频率：整个PWM周期的倒数</li>\n<li>分辨率：占空比变化步长 <h4 id=\"PWM实现方法\"><a href=\"#PWM实现方法\" class=\"headerlink\" title=\"PWM实现方法\"></a>PWM实现方法</h4>输出比较模式，依靠内部计数器cnt和ccr设置的数值的比较来进行输出电平的控制，常用的有匹配时电平翻转和PWM模式<br>PWM占空比：$$DutyCycle&#x3D;\\frac{CCR}{ARR}$$<br>PWM频率：$$Freq&#x3D;\\frac{F_{clk}}{ARR}$$<br>PWM分辨率：$$Resolution&#x3D;\\frac{ARR}{2^{n}}$$<h4 id=\"高级定时器\"><a href=\"#高级定时器\" class=\"headerlink\" title=\"高级定时器\"></a>高级定时器</h4>死区生成：可以避免推挽电路上下管同时打开导致短路<h3 id=\"PWM控制电机\"><a href=\"#PWM控制电机\" class=\"headerlink\" title=\"PWM控制电机\"></a>PWM控制电机</h3>舵机是根据pwm信号控制舵机转动角度的，内部有直流电机<h4 id=\"电机驱动芯片\"><a href=\"#电机驱动芯片\" class=\"headerlink\" title=\"电机驱动芯片\"></a>电机驱动芯片</h4>利用H桥，可以控制电机转动方向。四个开关管可以构成两个推挽电路，使得电机可以获得两个方向的电流。<br>电机需要的电源一般是大功率的，不能直接通过gpio驱动，因此可以通过让stlink的5v口接入电机驱动芯片来获得电源。但是注意，pwm信号的地应当和电机电源的地相连，否则会出现电平不稳定的情况。<h3 id=\"PWM代码\"><a href=\"#PWM代码\" class=\"headerlink\" title=\"PWM代码\"></a>PWM代码</h3>pwm的激活结构如下：<br><img src=\"/2023/05/11/%E7%94%B5%E8%B5%9B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%881%EF%BC%89/PWM_Structure.jpg\" alt=\"PWM_Structure\"></li>\n<li>RCC开启时钟</li>\n<li>配置时基单元</li>\n<li>配置输出比较单元</li>\n<li>配置GPIO，设置为复用推挽输出</li>\n<li>运行控制，启动计数器<h4 id=\"TIM库函数\"><a href=\"#TIM库函数\" class=\"headerlink\" title=\"TIM库函数\"></a>TIM库函数</h4>在hal库中，tim相关库函数在stm32f1xx_hal_tim.h文件中<br>其中有关输出比较的内容有：</li>\n<li>TIM_OC_InitTypeDef: 输出比较初始化结构体</li>\n<li>HAL_StatusTypeDef HAL_TIM_OC_Init(TIM_HandleTypeDef *htim)：输出比较初始化函数</li>\n<li>HAL_StatusTypeDef HAL_TIM_OC_ConfigChannel(TIM_HandleTypeDef *htim, TIM_OC_InitTypeDef *sConfig, uint32_t Channel)：配置输出通道函数</li>\n</ul>\n",
            "tags": [
                "技术",
                "博客",
                "电赛"
            ]
        },
        {
            "id": "http://example.com/2022/04/18/%E5%85%B3%E4%BA%8E%E8%A7%A3%E5%86%B3%E6%97%A0%E6%B3%95%E4%B8%8A%E4%BC%A0%E5%9B%BE%E7%89%87%E7%9A%84%E9%97%AE%E9%A2%98/",
            "url": "http://example.com/2022/04/18/%E5%85%B3%E4%BA%8E%E8%A7%A3%E5%86%B3%E6%97%A0%E6%B3%95%E4%B8%8A%E4%BC%A0%E5%9B%BE%E7%89%87%E7%9A%84%E9%97%AE%E9%A2%98/",
            "title": "关于解决无法上传图片的问题",
            "date_published": "2022-04-18T15:45:02.000Z",
            "content_html": "<h1 id=\"问题\"><a href=\"#问题\" class=\"headerlink\" title=\"问题\"></a>问题</h1><p>事情是这样的，我在写博客的时候发现，我的图片即使以正确的格式引用，依旧会出现无法加载的问题<br><img src=\"/2022/04/18/%E5%85%B3%E4%BA%8E%E8%A7%A3%E5%86%B3%E6%97%A0%E6%B3%95%E4%B8%8A%E4%BC%A0%E5%9B%BE%E7%89%87%E7%9A%84%E9%97%AE%E9%A2%98/problem.jpg\" alt=\"问题如图\"></p>\n<p>众所周知，没有图片，你写个啥都没法直观地展示，就好像pre时用txt做演示，大家嘴上不说什么，心里肯定知道<del>你是忘了做ppt了</del></p>\n<p>总之，根据我一晚上的研究成果，整理出来了几个解决图片无法显示的问题的方法供大家参考。</p>\n<h1 id=\"解决方法\"><a href=\"#解决方法\" class=\"headerlink\" title=\"解决方法\"></a>解决方法</h1><h2 id=\"目录\"><a href=\"#目录\" class=\"headerlink\" title=\"目录\"></a>目录</h2><p>一共有这么几个方法，大家可以都试试，挑一个自己最喜欢的</p>\n<ul>\n<li><p><a href=\"./#%E5%9B%BE%E5%BA%8A%E6%B3%95\">图床法</a></p>\n<ul>\n<li>github&amp;gitee图床</li>\n<li>图床网站</li>\n<li>PicGo</li>\n</ul>\n</li>\n<li><p><a href=\"./#%E6%9C%AC%E5%9C%B0%E4%B8%8A%E4%BC%A0%E6%B3%95\">本地上传法</a></p>\n</li>\n</ul>\n<h2 id=\"图床法\"><a href=\"#图床法\" class=\"headerlink\" title=\"图床法\"></a>图床法</h2><p>是这样的，一般来说，你的hexo博客在部署到服务器时，不会给你上传那些文章里链接的图片的，所以你的md文章里链接的图片一般情况下是无法上传的，自然就无法加载出来，但是你的图片如果是网络图片，直接链接网址，就可以通过联网加载的方式显示有如下几种方法</p>\n<h3 id=\"Github-amp-Gitee仓库图床\"><a href=\"#Github-amp-Gitee仓库图床\" class=\"headerlink\" title=\"Github&amp;Gitee仓库图床\"></a>Github&amp;Gitee仓库图床</h3><p>这个的原理就是让你的公有仓库变成图床，白嫖存储空间</p>\n<h4 id=\"操作流程\"><a href=\"#操作流程\" class=\"headerlink\" title=\"操作流程\"></a>操作流程</h4><p>具体来说，github和gitee方法相似，这里仅介绍github，gitee方法类似<br>gitee的访问速度会更快一点，github的容量没有限制，可以自己取舍</p>\n<ol>\n<li>注册一个github账户</li>\n<li>创建一个新的公有库，注意一定是<strong>公有</strong>，否则外部无法访问</li>\n<li>在库存中创建一个文件夹<br> <img src=\"/2022/04/18/%E5%85%B3%E4%BA%8E%E8%A7%A3%E5%86%B3%E6%97%A0%E6%B3%95%E4%B8%8A%E4%BC%A0%E5%9B%BE%E7%89%87%E7%9A%84%E9%97%AE%E9%A2%98/github.jpg\" alt=\"add_a_file\"></li>\n<li>把你的图片上传<br><img src=\"/2022/04/18/%E5%85%B3%E4%BA%8E%E8%A7%A3%E5%86%B3%E6%97%A0%E6%B3%95%E4%B8%8A%E4%BC%A0%E5%9B%BE%E7%89%87%E7%9A%84%E9%97%AE%E9%A2%98/upload.jpg\" alt=\"upload\"></li>\n<li>点击你的图片，复制地址框中的地址，注意要把bolb改为raw<br><img src=\"/2022/04/18/%E5%85%B3%E4%BA%8E%E8%A7%A3%E5%86%B3%E6%97%A0%E6%B3%95%E4%B8%8A%E4%BC%A0%E5%9B%BE%E7%89%87%E7%9A%84%E9%97%AE%E9%A2%98/raw.jpg\" alt=\"raw\"></li>\n<li>然后就可以在你的博客里引用这个地址了！<blockquote>\n<p>你可以用cdn加速github，比如Jsdelivr，加速方法不在本文讨论范围</p>\n</blockquote>\n</li>\n</ol>\n<h3 id=\"图床网站\"><a href=\"#图床网站\" class=\"headerlink\" title=\"图床网站\"></a>图床网站</h3><p>上面说的只是把github当作一个公开访问的图片网站，当然，市面上还有很多的专用图床网站，免费的付费的都有，这里介绍一个免费的网站<a href=\"https://imgtu.com/\">imgtu.com</a></p>\n<ol>\n<li>打开网站，上传图片<br><img src=\"/2022/04/18/%E5%85%B3%E4%BA%8E%E8%A7%A3%E5%86%B3%E6%97%A0%E6%B3%95%E4%B8%8A%E4%BC%A0%E5%9B%BE%E7%89%87%E7%9A%84%E9%97%AE%E9%A2%98/image.jpg\" alt=\"image\"><blockquote>\n<p>注意不能挂梯子</p>\n</blockquote>\n</li>\n<li>上传完成后，在底部链接栏，找到md链接，复制粘贴到你的文章插图位置就ok了<br><img src=\"/2022/04/18/%E5%85%B3%E4%BA%8E%E8%A7%A3%E5%86%B3%E6%97%A0%E6%B3%95%E4%B8%8A%E4%BC%A0%E5%9B%BE%E7%89%87%E7%9A%84%E9%97%AE%E9%A2%98/save_the_link.jpg\" alt=\"save_the_link\"></li>\n</ol>\n<p>这个方法还是比较简单的,基本上有手就行，没手的话，<del>那你也别搞博客了</del></p>\n<h3 id=\"PicGo\"><a href=\"#PicGo\" class=\"headerlink\" title=\"PicGo\"></a>PicGo</h3><p>除了以上介绍的两种方法，还有一个比较“软件化”的方案，就是<a href=\"https://molunerfinn.com/PicGo/\">PicGo</a></p>\n<p>PicGo是一个开源的软件，它的优点是方便快捷，不用登网站，操作比较easy，而且集成了很多平台。<br><img src=\"/2022/04/18/%E5%85%B3%E4%BA%8E%E8%A7%A3%E5%86%B3%E6%97%A0%E6%B3%95%E4%B8%8A%E4%BC%A0%E5%9B%BE%E7%89%87%E7%9A%84%E9%97%AE%E9%A2%98/PicGo.jpg\" alt=\"PicGo\"></p>\n<h4 id=\"操作方法\"><a href=\"#操作方法\" class=\"headerlink\" title=\"操作方法\"></a>操作方法</h4><p>还是以GitHub为例，首先我们进入<strong>图床设置</strong>-&gt;<strong>Github</strong><br><img src=\"/2022/04/18/%E5%85%B3%E4%BA%8E%E8%A7%A3%E5%86%B3%E6%97%A0%E6%B3%95%E4%B8%8A%E4%BC%A0%E5%9B%BE%E7%89%87%E7%9A%84%E9%97%AE%E9%A2%98/picgo_github.jpg\" alt=\"picgo_github\"><br>在对应的框里输入信息</p>\n<blockquote>\n<p>存储路径是你的GitHub仓库里的路径，没有时会创建<br>自定义域名就是你可以用cdn加速访问图片，最后两级就是你的用户名和仓库名<br>下面介绍一下token的获取方法</p>\n</blockquote>\n<h5 id=\"获取Github-Token\"><a href=\"#获取Github-Token\" class=\"headerlink\" title=\"获取Github Token\"></a>获取Github Token</h5><p>首先从个人列表进入settings<br><img src=\"/2022/04/18/%E5%85%B3%E4%BA%8E%E8%A7%A3%E5%86%B3%E6%97%A0%E6%B3%95%E4%B8%8A%E4%BC%A0%E5%9B%BE%E7%89%87%E7%9A%84%E9%97%AE%E9%A2%98/token1.jpg\" alt=\"token1\"><br>然后进入最底部的developer settings<br><img src=\"/2022/04/18/%E5%85%B3%E4%BA%8E%E8%A7%A3%E5%86%B3%E6%97%A0%E6%B3%95%E4%B8%8A%E4%BC%A0%E5%9B%BE%E7%89%87%E7%9A%84%E9%97%AE%E9%A2%98/token2.jpg\" alt=\"token2\"><br>然后进入Personal access tokens，点generate new token<br><img src=\"/2022/04/18/%E5%85%B3%E4%BA%8E%E8%A7%A3%E5%86%B3%E6%97%A0%E6%B3%95%E4%B8%8A%E4%BC%A0%E5%9B%BE%E7%89%87%E7%9A%84%E9%97%AE%E9%A2%98/token3.jpg\" alt=\"token3\"><br>按照如下操作<br><img src=\"/2022/04/18/%E5%85%B3%E4%BA%8E%E8%A7%A3%E5%86%B3%E6%97%A0%E6%B3%95%E4%B8%8A%E4%BC%A0%E5%9B%BE%E7%89%87%E7%9A%84%E9%97%AE%E9%A2%98/token4.jpg\" alt=\"token4\"><br>生成后记得复制，这个token<font color=\"Red\" size=\"6\"><strong>只会出现一次</strong></font></p>\n<p>至此，PicGo的使用介绍就完成了</p>\n<h2 id=\"本地上传法\"><a href=\"#本地上传法\" class=\"headerlink\" title=\"本地上传法\"></a>本地上传法</h2><h2 id=\"（施工中，累了，明天再说）\"><a href=\"#（施工中，累了，明天再说）\" class=\"headerlink\" title=\"（施工中，累了，明天再说）\"></a>（施工中，累了，明天再说）</h2><p>2022.4.20更新</p>\n<p>继续说本地上传法</p>\n<p>我们之前说过，本地的图片是不会被hexo上传的，其实这个说法不严谨<br>严格来说，是你凭空放一张图片，无法上传<br>但是，我们可以通过一个方法来上传本地图片，那就是hexo-asset-image。</p>\n<h3 id=\"操作方法-1\"><a href=\"#操作方法-1\" class=\"headerlink\" title=\"操作方法\"></a>操作方法</h3><p>首先安装hexo-asset-image<br><code>npm install https://github.com/CodeFalling/hexo-asset-image --save</code></p>\n<ul>\n<li>注意，如果你安装速度慢的话，可以讲npm换为淘宝镜像，切换方法如下：<br><code>npm config set registry https://registry.npm.taobao.org</code><br>安装完成后，我们要在_config.yml中作如下更改<blockquote>\n<p> 将 post_asset_folder 设置为true</p>\n</blockquote>\n</li>\n</ul>\n<p><img src=\"/2022/04/18/%E5%85%B3%E4%BA%8E%E8%A7%A3%E5%86%B3%E6%97%A0%E6%B3%95%E4%B8%8A%E4%BC%A0%E5%9B%BE%E7%89%87%E7%9A%84%E9%97%AE%E9%A2%98/post.jpg\" alt=\"post\"><br>然后，每当我们新建一篇博客时，就会有一个同名文件夹在_post文件夹中生成了<br>我们把需要插入的图片放到这个文件夹里面，在文章中引用格式如下<br><code>![图片描述]（./包名/NO.01.001.jpg）</code><br><font size=\"5\">或者</font><br><code>![logo](logo.jpg)</code><br>就可以了，这个方法也是我在用的方法，非常方便，缺点是对服务器压力比较大。</p>\n<h1 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h1><p>上述的几个方法，各有优缺点，可以结合自己的特点来使用<br>注意图片描述必须是全英文，否则无法显示图片<br>希望有所帮助</p>\n",
            "tags": [
                "技术",
                "博客",
                "markdown"
            ]
        },
        {
            "id": "http://example.com/2022/04/18/Hexo%E4%B8%BB%E9%A2%98%E6%A8%A1%E6%9D%BF%E5%88%87%E6%8D%A2/",
            "url": "http://example.com/2022/04/18/Hexo%E4%B8%BB%E9%A2%98%E6%A8%A1%E6%9D%BF%E5%88%87%E6%8D%A2/",
            "title": "Hexo主题模板切换",
            "date_published": "2022-04-18T12:56:34.000Z",
            "content_html": "<h1 id=\"下载主题\"><a href=\"#下载主题\" class=\"headerlink\" title=\"下载主题\"></a>下载主题</h1><p>首先，我们找一个比较好看的主题，比如我找的Fluid<br><img src=\"/2022/04/18/Hexo%E4%B8%BB%E9%A2%98%E6%A8%A1%E6%9D%BF%E5%88%87%E6%8D%A2/fluid.jpg\" alt=\"fluid网址：https://hexo.fluid-dev.com \"></p>\n<p>找到了这个主题的<a href=\"https://github.com/fluid-dev/hexo-theme-fluid\">github网址</a> </p>\n<p>然后呢，我们需要在cmd中输入一行神秘代码<br><code>git clone https://github.com/fluid-dev/hexo-theme-fluid themes\\fluid</code></p>\n<p>git clone 是在GitHub上下载的命令，中间的部分是这个主题的网址，最后是你在blog目录下需要把这个下载的主题存到的位置，系统会自动创建空的文件夹。</p>\n<p>然后静待下载，下载完成后，我们的工作就成功了一大半了！</p>\n<h1 id=\"应用主题\"><a href=\"#应用主题\" class=\"headerlink\" title=\"应用主题\"></a>应用主题</h1><p>应用主题的方法很简单，只需要打开blog目录下的_config.yml文件，把倒数第二个部分的“theme：”改为你的主题所在文件夹的名字就OK了。<br><img src=\"/2022/04/18/Hexo%E4%B8%BB%E9%A2%98%E6%A8%A1%E6%9D%BF%E5%88%87%E6%8D%A2/config.jpg\" alt=\"这样\"></p>\n<p>别忘了部署到服务器！</p>\n",
            "tags": [
                "技术",
                "博客",
                "markdown"
            ]
        },
        {
            "id": "http://example.com/2021/08/24/md%E8%AF%AD%E6%B3%95%E8%AF%95%E9%AA%8C/",
            "url": "http://example.com/2021/08/24/md%E8%AF%AD%E6%B3%95%E8%AF%95%E9%AA%8C/",
            "title": "md语法试验",
            "date_published": "2021-08-24T07:32:52.000Z",
            "content_html": "<h1 id=\"分层\"><a href=\"#分层\" class=\"headerlink\" title=\"分层\"></a>分层</h1><h2 id=\"二级目录\"><a href=\"#二级目录\" class=\"headerlink\" title=\"二级目录\"></a>二级目录</h2><h3 id=\"列表\"><a href=\"#列表\" class=\"headerlink\" title=\"列表\"></a>列表</h3><h4 id=\"无序列表\"><a href=\"#无序列表\" class=\"headerlink\" title=\"无序列表\"></a>无序列表</h4><ul>\n<li>小标</li>\n<li>无序</li>\n<li>各种符号都行<ul>\n<li>第二层嵌套<ul>\n<li>第n层嵌套<h4 id=\"有序列表\"><a href=\"#有序列表\" class=\"headerlink\" title=\"有序列表\"></a>有序列表</h4></li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<ol>\n<li>有序列表</li>\n<li>第一行序号为起始序号</li>\n<li>即使后面序号错误也会顺序下排<ol>\n<li>嵌套效果<h2 id=\"内容\"><a href=\"#内容\" class=\"headerlink\" title=\"内容\"></a>内容</h2><h3 id=\"引用说明\"><a href=\"#引用说明\" class=\"headerlink\" title=\"引用说明\"></a>引用说明</h3><blockquote>\n<p>引用内容</p>\n<blockquote>\n<p>二级引用</p>\n<blockquote>\n<p>三级引用</p>\n</blockquote>\n</blockquote>\n</blockquote>\n<h3 id=\"代码块\"><a href=\"#代码块\" class=\"headerlink\" title=\"代码块\"></a>代码块</h3><p><code>少量代码，单行使用，用·包裹</code></p>\n</li>\n</ol>\n</li>\n</ol>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><div class=\"code-wrapper\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></div></td><td class=\"code\"><pre><code class=\"hljs\"><br>大量代码多行使用，用三个·包裹<br>大量代码多行使用，用三个·包裹<br>大量代码多行使用，用三个·包裹<br>大量代码多行使用，用三个·包裹<br><br></code></pre></td></tr></table></figure>\n\n<h2 id=\"链接\"><a href=\"#链接\" class=\"headerlink\" title=\"链接\"></a>链接</h2><h3 id=\"网页链接\"><a href=\"#网页链接\" class=\"headerlink\" title=\"网页链接\"></a>网页链接</h3><h4 id=\"行内式\"><a href=\"#行内式\" class=\"headerlink\" title=\"行内式\"></a>行内式</h4><p>链接放在【】中，地址放在后面的小括号中，引号内是title<br><a href=\"www.baidu.com\" title=\"百度一下，你就知道\">百度</a><br>[百度]是一个搜索引擎</p>\n<h4 id=\"参数式\"><a href=\"#参数式\" class=\"headerlink\" title=\"参数式\"></a>参数式</h4><p>链接在【】内，地址在冒号后面，title用引号<br>[百度]:<a href=\"http://www.baidu.com/\">www.baidu.com</a> “百度一下，你就知道”<br>[百度]是一个搜索引擎</p>\n<h3 id=\"图片\"><a href=\"#图片\" class=\"headerlink\" title=\"图片\"></a>图片</h3><p>与链接基本一致，注意在引用图片时【】前加上！<br><img src=\"/2021/08/24/md%E8%AF%AD%E6%B3%95%E8%AF%95%E9%AA%8C/download\\edge\\13623636-6d878e3d3ef63825\" alt=\"logo\"> “my logo”</p>\n<h2 id=\"工整\"><a href=\"#工整\" class=\"headerlink\" title=\"工整\"></a>工整</h2><h3 id=\"分割线\"><a href=\"#分割线\" class=\"headerlink\" title=\"分割线\"></a>分割线</h3><h2 id=\"由-这三种之一的三个符号表示\"><a href=\"#由-这三种之一的三个符号表示\" class=\"headerlink\" title=\"由* - _这三种之一的三个符号表示\"></a>由* - _这三种之一的三个符号表示</h2><p>这就是分割线</p>\n<h3 id=\"表格\"><a href=\"#表格\" class=\"headerlink\" title=\"表格\"></a>表格</h3><p>&#x2F;&#x2F;例子</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">123</th>\n<th align=\"center\">234</th>\n<th align=\"right\">345</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">abc</td>\n<td align=\"center\">bcd</td>\n<td align=\"right\">cde</td>\n</tr>\n</tbody></table>\n<h2 id=\"其他\"><a href=\"#其他\" class=\"headerlink\" title=\"其他\"></a>其他</h2><h3 id=\"强调字体\"><a href=\"#强调字体\" class=\"headerlink\" title=\"强调字体\"></a>强调字体</h3><ol>\n<li>强调字体<br> 用星号包裹，如<em>md</em>,<strong>md</strong> </li>\n<li>转义<br> 用\\</li>\n<li>删除线<br> <del>删除</del></li>\n</ol>\n",
            "tags": [
                "技术",
                "博客",
                "markdown"
            ]
        }
    ]
}