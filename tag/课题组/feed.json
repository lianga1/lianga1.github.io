{
    "version": "https://jsonfeed.org/version/1",
    "title": "意大利炮打友军 • All posts by \"课题组\" tag",
    "description": "",
    "home_page_url": "http://example.com",
    "items": [
        {
            "id": "http://example.com/2023/11/15/%E8%AF%BE%E9%A2%98%E7%BB%8423-11-15%E5%91%A8%E6%8A%A5/",
            "url": "http://example.com/2023/11/15/%E8%AF%BE%E9%A2%98%E7%BB%8423-11-15%E5%91%A8%E6%8A%A5/",
            "title": "课题组23-11-15周报",
            "date_published": "2023-11-15T08:09:06.000Z",
            "content_html": "<h1 id=\"实验关于fp16参数转换速度的问题\"><a href=\"#实验关于fp16参数转换速度的问题\" class=\"headerlink\" title=\"实验关于fp16参数转换速度的问题\"></a>实验关于fp16参数转换速度的问题</h1><p>本周进行了一个实验，主要用于观察pytorch中对张量转移的各种方法的性能差异。</p>\n<h2 id=\"实验思路\"><a href=\"#实验思路\" class=\"headerlink\" title=\"实验思路\"></a>实验思路</h2><h3 id=\"几种不同的传输方向\"><a href=\"#几种不同的传输方向\" class=\"headerlink\" title=\"几种不同的传输方向\"></a>几种不同的传输方向</h3><ul>\n<li>cpu -&gt; cpu</li>\n<li>cpu -&gt; gpu</li>\n<li>gpu -&gt; cpu</li>\n<li>gpu -&gt; gpu<h3 id=\"几种不同的数据\"><a href=\"#几种不同的数据\" class=\"headerlink\" title=\"几种不同的数据\"></a>几种不同的数据</h3></li>\n<li>fp32 -&gt; fp32</li>\n<li>fp32 -&gt; fp16<br><code>调用half()函数，将fp32数据转换为fp16数据</code></li>\n<li>fp16 -&gt; fp16</li>\n<li>fp16 -&gt; fp32<br><code>调用float()函数，将fp16数据转换为fp32数据</code></li>\n</ul>\n<p><strong>目前第三、四种暂未测试</strong></p>\n<h3 id=\"几种不同的传输方式\"><a href=\"#几种不同的传输方式\" class=\"headerlink\" title=\"几种不同的传输方式\"></a>几种不同的传输方式</h3><ul>\n<li>copy_()</li>\n<li>to()</li>\n</ul>\n<h2 id=\"实验过程\"><a href=\"#实验过程\" class=\"headerlink\" title=\"实验过程\"></a>实验过程</h2><p>准备一个目的地矩阵，一个源矩阵组（100个）。分别用随机数初始化。<br>循环100次，每次都遍历整个矩阵组，传输至对应的目的地矩阵。<br>测量总时长，对不同情况进行比较<br>代码模板如下：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><div class=\"code-wrapper\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></div></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-keyword\">import</span> torch<br><span class=\"hljs-keyword\">import</span> time<br>tensor_cpu_1 = torch.rand(<span class=\"hljs-number\">1000</span>, <span class=\"hljs-number\">1000</span>)<br>tensor_gpu_1 = torch.rand(<span class=\"hljs-number\">1000</span>,<span class=\"hljs-number\">1000</span>).cuda()<br>tensor_cpu_2 = torch.rand(<span class=\"hljs-number\">1000</span>, <span class=\"hljs-number\">1000</span>,<span class=\"hljs-number\">1000</span>)<br>tensor_gpu_2 = torch.rand(<span class=\"hljs-number\">1000</span>,<span class=\"hljs-number\">1000</span>,<span class=\"hljs-number\">1000</span>).cuda()<br>time_sum = <span class=\"hljs-number\">0</span><br><span class=\"hljs-keyword\">for</span> j <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">range</span>(<span class=\"hljs-number\">100</span>):<br>    start = time.time()<br>    <span class=\"hljs-keyword\">for</span> i <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">range</span>(<span class=\"hljs-number\">100</span>):<br>        tensor_cpu_1.copy_(tensor_cpu_2[i])<br>    end = time.time()<br>    time_sum += end - start<br><br><br><span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">&quot;time for copy from cpu to cpu via _copy():&quot;</span>, time_sum)<br><br></code></pre></td></tr></table></figure>\n<p>如上代码展示了从cpu到cpu传输fp32的过程。最终展示了传输十万个1000*1000的矩阵所耗费的总时间。<br>经过实验，结果如下表所示：<br>记录数据如下：</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">传输方向</th>\n<th align=\"center\">传输方式</th>\n<th align=\"center\">数据类型</th>\n<th align=\"center\">时间</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\">cpu -&gt; cpu</td>\n<td align=\"center\">copy_()</td>\n<td align=\"center\">fp32 -&gt; fp32</td>\n<td align=\"center\">2.187</td>\n</tr>\n<tr>\n<td align=\"center\">cpu -&gt; cpu</td>\n<td align=\"center\">to()</td>\n<td align=\"center\">fp32 -&gt; fp32</td>\n<td align=\"center\">0.025</td>\n</tr>\n<tr>\n<td align=\"center\">cpu -&gt; cpu</td>\n<td align=\"center\">copy_()</td>\n<td align=\"center\">fp32 -&gt; fp16</td>\n<td align=\"center\">5.855</td>\n</tr>\n<tr>\n<td align=\"center\">cpu -&gt; cpu</td>\n<td align=\"center\">to()</td>\n<td align=\"center\">fp32 -&gt; fp16</td>\n<td align=\"center\">5.634</td>\n</tr>\n<tr>\n<td align=\"center\">cpu -&gt; gpu</td>\n<td align=\"center\">copy_()</td>\n<td align=\"center\">fp32 -&gt; fp32</td>\n<td align=\"center\">9.663</td>\n</tr>\n<tr>\n<td align=\"center\">cpu -&gt; gpu</td>\n<td align=\"center\">to()</td>\n<td align=\"center\">fp32 -&gt; fp32</td>\n<td align=\"center\">9.555</td>\n</tr>\n<tr>\n<td align=\"center\">cpu -&gt; gpu</td>\n<td align=\"center\">copy_()</td>\n<td align=\"center\">fp32 -&gt; fp16</td>\n<td align=\"center\">9.876</td>\n</tr>\n<tr>\n<td align=\"center\">cpu -&gt; gpu</td>\n<td align=\"center\">to()</td>\n<td align=\"center\">fp32 -&gt; fp16</td>\n<td align=\"center\">10.264</td>\n</tr>\n<tr>\n<td align=\"center\">gpu -&gt; cpu</td>\n<td align=\"center\">copy_()</td>\n<td align=\"center\">fp32 -&gt; fp32</td>\n<td align=\"center\">8.895</td>\n</tr>\n<tr>\n<td align=\"center\">gpu -&gt; cpu</td>\n<td align=\"center\">to()</td>\n<td align=\"center\">fp32 -&gt; fp32</td>\n<td align=\"center\">13.649</td>\n</tr>\n<tr>\n<td align=\"center\">gpu -&gt; cpu</td>\n<td align=\"center\">copy_()</td>\n<td align=\"center\">fp32 -&gt; fp16</td>\n<td align=\"center\">10.051</td>\n</tr>\n<tr>\n<td align=\"center\">gpu -&gt; cpu</td>\n<td align=\"center\">to()</td>\n<td align=\"center\">fp32 -&gt; fp16</td>\n<td align=\"center\">7.320</td>\n</tr>\n<tr>\n<td align=\"center\">gpu -&gt; gpu</td>\n<td align=\"center\">copy_()</td>\n<td align=\"center\">fp32 -&gt; fp32</td>\n<td align=\"center\">0.605</td>\n</tr>\n<tr>\n<td align=\"center\">gpu -&gt; gpu</td>\n<td align=\"center\">to()</td>\n<td align=\"center\">fp32 -&gt; fp32</td>\n<td align=\"center\">0.029</td>\n</tr>\n<tr>\n<td align=\"center\">gpu -&gt; gpu</td>\n<td align=\"center\">copy_()</td>\n<td align=\"center\">fp32 -&gt; fp16</td>\n<td align=\"center\">0.484</td>\n</tr>\n<tr>\n<td align=\"center\">gpu -&gt; gpu</td>\n<td align=\"center\">to()</td>\n<td align=\"center\">fp32 -&gt; fp16</td>\n<td align=\"center\">0.591</td>\n</tr>\n</tbody></table>\n<ul>\n<li>注意，测试时间可能会有波动，尤其是在时间较短时，考虑到这种传输主要出现在gpu-&gt;gpu中，不是主要考虑内容</li>\n</ul>\n",
            "tags": [
                "技术",
                "周报",
                "课题组",
                "神经网络",
                "pytorch"
            ]
        },
        {
            "id": "http://example.com/2023/10/22/%E8%AF%BE%E9%A2%98%E7%BB%84%E7%AC%AC%E4%BA%94%E5%91%A8%E5%AD%A6%E4%B9%A0/",
            "url": "http://example.com/2023/10/22/%E8%AF%BE%E9%A2%98%E7%BB%84%E7%AC%AC%E4%BA%94%E5%91%A8%E5%AD%A6%E4%B9%A0/",
            "title": "课题组第五周学习",
            "date_published": "2023-10-22T15:36:49.000Z",
            "content_html": "<h1 id=\"高效直接访问主机内存的方法\"><a href=\"#高效直接访问主机内存的方法\" class=\"headerlink\" title=\"高效直接访问主机内存的方法\"></a>高效直接访问主机内存的方法</h1><h2 id=\"现有方法存在的问题\"><a href=\"#现有方法存在的问题\" class=\"headerlink\" title=\"现有方法存在的问题\"></a>现有方法存在的问题</h2><h3 id=\"通过加载后执行的方法\"><a href=\"#通过加载后执行的方法\" class=\"headerlink\" title=\"通过加载后执行的方法\"></a>通过加载后执行的方法</h3><p>面对巨大的模型参数规模，现有GPU的显存难以支撑大模型的训练。因此产生了一种通过加载后执行的方法，即将模型参数存储在主机内存中，每次训练时将参数加载到显存中，训练结束后将参数保存到主机内存中。这种方法的缺点是每次训练都需要将参数加载到显存中，这个过程会消耗大量的时间，例如在v100上，加载时间会是处理时间的4倍以上，导致训练效率低下。有一种异步加载方法，将加载层和训练层分开，训练层在训练时异步加载参数，但是这种方法会导致训练时的显存占用过高，而且层数较多时加载时间过高的劣势逐渐显现，优化并不明显。</p>\n<h2 id=\"本文提出的方法\"><a href=\"#本文提出的方法\" class=\"headerlink\" title=\"本文提出的方法\"></a>本文提出的方法</h2><h3 id=\"直接主机访问\"><a href=\"#直接主机访问\" class=\"headerlink\" title=\"直接主机访问\"></a>直接主机访问</h3><p>避开加载和训练不同步的问题，直接将cpu内存当作gpu的虚拟内存进行访问，这样避免了加载过程中占用gpu显存过高的问题，但是由于访问和数据流动要经过pcie总线，传输速度较慢。<br>因此，DHA使用了这样一种办法，使得其可以自适应选择访问方式，其可以通过直接主机访问和加载后执行两种方法进行训练，使得加载的时间可以隐藏在训练流的流水线中。</p>\n<h3 id=\"多GPU方法\"><a href=\"#多GPU方法\" class=\"headerlink\" title=\"多GPU方法\"></a>多GPU方法</h3><p>对于多个GPU，由于GPU间通信效率要高于PCIE通信效率，因此可以将模型拆分成多个部分，分别存储在不同的GPU中，这样每次训练的加载都可以直接从其他GPU中加载，而不需要从主机内存中加载，这样可以减少加载时间。</p>\n<h3 id=\"DeepPlan\"><a href=\"#DeepPlan\" class=\"headerlink\" title=\"DeepPlan\"></a>DeepPlan</h3><p>本文还提出了一个工具：用来为给定模型自动生成执行计划，过程如下：</p>\n<ul>\n<li>对本地GPU显存和主机内存分析性能</li>\n<li>通过比较DHA和流水线方法的延迟差异来决定每一层的策略</li>\n<li>如果有多个GPU，则根据GPU数量平均划分模型</li>\n<li>协调将直接主机访问的执行和加载后执行的执行进行协调<br>本方案在部署时只需要进行一次执行。<h2 id=\"原理分析\"><a href=\"#原理分析\" class=\"headerlink\" title=\"原理分析\"></a>原理分析</h2>对于不同层，加载——执行策略与DHA策略的时间是不同的，<table>\n<thead>\n<tr>\n<th align=\"center\">层</th>\n<th align=\"center\">加载——执行策略</th>\n<th align=\"center\">DHA策略</th>\n<th align=\"center\">结论</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\">卷积层</td>\n<td align=\"center\">小规模差异不大</td>\n<td align=\"center\">大规模较慢</td>\n<td align=\"center\">推荐在较小卷积层使用DHA，同时加载较大卷积层等待直接执行</td>\n</tr>\n<tr>\n<td align=\"center\">全连接层</td>\n<td align=\"center\">加载快</td>\n<td align=\"center\">执行慢</td>\n<td align=\"center\">推荐在全连接层使用加载后执行，因为其需要频繁访问内存</td>\n</tr>\n<tr>\n<td align=\"center\">嵌入层</td>\n<td align=\"center\">加载较慢</td>\n<td align=\"center\">执行较快</td>\n<td align=\"center\">推荐在嵌入层使用DHA，因为其规模较大，而层中一些参数的访问较少</td>\n</tr>\n<tr>\n<td align=\"center\">归一化层</td>\n<td align=\"center\">LayerNorm更好</td>\n<td align=\"center\">BatchNorm更好</td>\n<td align=\"center\">需要根据具体情况进行选择</td>\n</tr>\n</tbody></table>\n</li>\n</ul>\n<p>产生差异的原因则是不同层对内存访问的需求不同，导致pcie访问次数不同，pcie作为瓶颈，访问次数越多，延迟越大。</p>\n<h3 id=\"并行传输\"><a href=\"#并行传输\" class=\"headerlink\" title=\"并行传输\"></a>并行传输</h3><p>对于多GPU场景，将模型划分为多个部分后，采用并行传输策略：从内存并行地将模型传输到两个GPU，再从第二个GPU向第一个GPU传输，这样可以减少传输时间。<br>在此基础上，可以将GPU2——GPU1的传输变为流水线传输，这样可以进一步减少传输时间。<br>但是，由于CPU提供的PCIE总线数量限制，多GPU系统，例如8GPU也只能每两个GPU公用一组总线，因此多GPU的总线需要考虑总线拓扑。</p>\n<h2 id=\"DeepPlan实现\"><a href=\"#DeepPlan实现\" class=\"headerlink\" title=\"DeepPlan实现\"></a>DeepPlan实现</h2><h3 id=\"整体实现思路\"><a href=\"#整体实现思路\" class=\"headerlink\" title=\"整体实现思路\"></a>整体实现思路</h3><p>再进行训练前，deepPlan会根据每一层的性能分析，推理出当前层采用何种方式进行训练（加载——执行orDHA）。遍历完整个网络后，将根据策略直接执行训练。如果在多GPU系统中，DeepPlan还会根据GPU连连接拓扑，将模型划分为多个部分，应用并行传输方案。</p>\n<h3 id=\"单层性能分析\"><a href=\"#单层性能分析\" class=\"headerlink\" title=\"单层性能分析\"></a>单层性能分析</h3><p>利用单层执行时间的统计数据，或者执行一次单层来得到每一层的性能数据。</p>\n<h3 id=\"层间性能分析\"><a href=\"#层间性能分析\" class=\"headerlink\" title=\"层间性能分析\"></a>层间性能分析</h3><p>对于每层性能已经得到的情况。检查每一层切换策略到DHA后其获得的性能差异是否比加载后执行的停滞时间更短，如果是的话则切换为DHA。并且通过递归的方式检查每个层之前最多可以使用几个DHA来缩短总加载停滞时间。</p>\n<h3 id=\"模型传输规划\"><a href=\"#模型传输规划\" class=\"headerlink\" title=\"模型传输规划\"></a>模型传输规划</h3><p>DeepPlan根据GPU拓扑，和PCIE交换机布局，避免并行加载的总线冲突，检查所选GPU是否使用NVLink，如果使用则直接进行并行传输，否则使用流水线传输。同时，根据并行传输带来的性能优化，重新规划每一层使用的策略。</p>\n",
            "tags": [
                "技术",
                "周报",
                "课题组",
                "神经网络",
                "pytorch"
            ]
        },
        {
            "id": "http://example.com/2023/10/13/%E8%AF%BE%E9%A2%98%E7%BB%84%E7%AC%AC%E5%9B%9B%E5%91%A8%E5%AD%A6%E4%B9%A0/",
            "url": "http://example.com/2023/10/13/%E8%AF%BE%E9%A2%98%E7%BB%84%E7%AC%AC%E5%9B%9B%E5%91%A8%E5%AD%A6%E4%B9%A0/",
            "title": "课题组第四周学习",
            "date_published": "2023-10-13T10:23:34.000Z",
            "content_html": "<h1 id=\"ZeRO-Offload方法\"><a href=\"#ZeRO-Offload方法\" class=\"headerlink\" title=\"ZeRO-Offload方法\"></a>ZeRO-Offload方法</h1><h2 id=\"提出背景\"><a href=\"#提出背景\" class=\"headerlink\" title=\"提出背景\"></a>提出背景</h2><p>对大模型训练来说，GPU显存对参数规模巨大的网络来说是一个瓶颈，然而CPU内存可以做到TB级别，因此可以考虑将一部分参数放在CPU上，而将需要频繁访问的参数放在GPU上，这样可以减少GPU显存的压力，提高训练速度。ZeRO-Offload提出了一种没有数据冗余的优化方法，可以将模型参数分布在CPU和GPU上，而且可以在CPU和GPU之间进行无缝的迁移。</p>\n<h3 id=\"大模型传统方法\"><a href=\"#大模型传统方法\" class=\"headerlink\" title=\"大模型传统方法\"></a>大模型传统方法</h3><p>针对大模型需要的内存过大的问题，传统分为两种方法：</p>\n<ul>\n<li>模型分割：将模型分割成多个部分，每个部分在GPU上训练，然后将结果传递给下一个部分，</li>\n<li>流水线并行：将训练过程分为不同层，每个层分给不同的GPU，然后将结果传递给下一个GPU<h2 id=\"增益来源\"><a href=\"#增益来源\" class=\"headerlink\" title=\"增益来源\"></a>增益来源</h2>根据计算流程，CPU的计算量相比于GPU的$O(MB)$,只有$O(M)$，其中M是模型大小，B是批次大小。<br>这个过程中，ZeRO-Offload将前向与后向传播分配给了GPU，而标准化计算和权重更新等对模型大小有直接联系的计算则分配给了CPU。<br>在数据吞吐方面，cpu与gpu之间仅存在fp16数据的传输，相比与其他方法（例如L2L）有大幅度减少<br>在并行方面，随着计算节点的增加，CPU的计算资源会随着节点数量增加而增加<br>CPU计算通过提高并行性增加了效率</li>\n</ul>\n<h3 id=\"对CPU作为计算瓶颈的解决方法\"><a href=\"#对CPU作为计算瓶颈的解决方法\" class=\"headerlink\" title=\"对CPU作为计算瓶颈的解决方法\"></a>对CPU作为计算瓶颈的解决方法</h3><h4 id=\"对CPU计算的优化\"><a href=\"#对CPU计算的优化\" class=\"headerlink\" title=\"对CPU计算的优化\"></a>对CPU计算的优化</h4><ul>\n<li>向量运算SIMD</li>\n<li>循环展开</li>\n<li>多核并行</li>\n<li>减少缓存抖动<h4 id=\"延迟参数更新\"><a href=\"#延迟参数更新\" class=\"headerlink\" title=\"延迟参数更新\"></a>延迟参数更新</h4>将参数更新延迟，重叠CPU与GPU计算。也就是说，在某一轮计算之后，此后每次gpu使用的优化器参数都是上一轮计算的结果，而不是这一轮计算的结果。，因此可以让cpu计算时间和gpu计算时间重叠。提高流水线负载率。<h2 id=\"优化方法\"><a href=\"#优化方法\" class=\"headerlink\" title=\"优化方法\"></a>优化方法</h2>ZeRO-Offload 同时利用CPU内存计算能力来优化。基于ZeRO优化方法，但是不是像原本多个GPU并行计算，并且通过联系收集器来进行并行。而是把这个通讯过程转化为与CPU的联系，相当于原本多个GPU同时做的工作，让单个GPU进行，每个阶段只进行原先一个GPU进行的工作，同时把其他GPU本应进行的计算状态经由内存进行存储。<h3 id=\"ZeRO的工作\"><a href=\"#ZeRO的工作\" class=\"headerlink\" title=\"ZeRO的工作\"></a>ZeRO的工作</h3>ZeRO，在ZeRO-Offload中使用ZeRO-2阶段，这个阶段你主要是分割模型状态和梯度。在ZeRO-2中，每个GPU都存储着所有参数，但是每轮训练只更新其中不包含的部分。<br>这个过程如下：</li>\n</ul>\n<ol>\n<li>每个GPU进行前馈，计算不同批次的损失。</li>\n<li>每个cpu进行反向传播，并且对每个有梯度的GPU使用减少梯度的算子进行平均。</li>\n<li>反向传播结束后，GPU使用其对应的梯度平均值对其部分参数和优化器状态进行更新。</li>\n<li>进行一次全收集，接收其他GPU计算的参数更新。</li>\n</ol>\n<h3 id=\"ZeRO-Offload的工作\"><a href=\"#ZeRO-Offload的工作\" class=\"headerlink\" title=\"ZeRO-Offload的工作\"></a>ZeRO-Offload的工作</h3><p>ZeRO-Offload将训练修改为数据流图，主要优势：使得CPU计算量减少了几个数量级。保证CPU与GPU通讯最小化。最大限度节省内存。</p>\n<h4 id=\"计算流图\"><a href=\"#计算流图\" class=\"headerlink\" title=\"计算流图\"></a>计算流图</h4><p>计算流图是一种图形化的表示，用于表示计算过程中的数据流动。在计算流图中，节点表示计算，边表示数据流动。<br><img src=\"https://raw.githubusercontent.com/lianga1/picGo_test/main/1.jpg\" alt=\"计算流图\"></p>\n<h4 id=\"减少CPU计算\"><a href=\"#减少CPU计算\" class=\"headerlink\" title=\"减少CPU计算\"></a>减少CPU计算</h4><p>ZeRO-Offload将前向与后向传播分配给了GPU，而标准化计算和权重更新等对模型大小有直接联系的计算则分配给了CPU。</p>\n<h4 id=\"减少CPU与GPU通讯\"><a href=\"#减少CPU与GPU通讯\" class=\"headerlink\" title=\"减少CPU与GPU通讯\"></a>减少CPU与GPU通讯</h4><p>创建fp32区：为了避免fp32数据在pcie总线传输，需要将所有fp32数据放在一个设备上进行处理<br>p16分配：将fp16必须放在前馈与反向传播共同节点的位置，因为这两个节点之间的通信是较大的。<br>因此，ZeRO-Offload将fp16分配给GPU，而将fp32分配给CPU。</p>\n<h4 id=\"减少内存\"><a href=\"#减少内存\" class=\"headerlink\" title=\"减少内存\"></a>减少内存</h4><p>将反向传播后得到的梯度，以及更新梯度所需要的计算和存储空间，写遭到CPU上，可以节省最多的显存使用。</p>\n<h2 id=\"优势\"><a href=\"#优势\" class=\"headerlink\" title=\"优势\"></a>优势</h2><h3 id=\"扩展性强\"><a href=\"#扩展性强\" class=\"headerlink\" title=\"扩展性强\"></a>扩展性强</h3><p>对于任何模型，其优化算法的优化参数对于ZeRO-Offload来说并不关键，其只是需要把fp32的计算内容单独放在CPU中。</p>\n<h3 id=\"支持并行\"><a href=\"#支持并行\" class=\"headerlink\" title=\"支持并行\"></a>支持并行</h3><p>对多个GPU而言。ZeRO-Offload基于ZeRO-2，因此可以将分区的参数分配给多个GPU。</p>\n<h3 id=\"模型并行\"><a href=\"#模型并行\" class=\"headerlink\" title=\"模型并行\"></a>模型并行</h3><p>ZeRO-Offload还可以用模型并行来实现更好的并行性。通过给cpu卸载梯度、优化器状态和优化器计算来和模型并行计算相适应。在这个情况下，首先，借由更难耗尽内存，可以使用更大的批次大小。其次，可以使用更多的GPU来进行模型并行计算。</p>\n",
            "tags": [
                "技术",
                "周报",
                "课题组",
                "神经网络",
                "pytorch"
            ]
        },
        {
            "id": "http://example.com/2023/09/30/%E8%AF%BE%E9%A2%98%E7%BB%84%E7%AC%AC%E4%B8%80%E5%91%A8%E5%AD%A6%E4%B9%A0/",
            "url": "http://example.com/2023/09/30/%E8%AF%BE%E9%A2%98%E7%BB%84%E7%AC%AC%E4%B8%80%E5%91%A8%E5%AD%A6%E4%B9%A0/",
            "title": "课题组第一周学习",
            "date_published": "2023-09-30T11:51:44.000Z",
            "content_html": "<h1 id=\"理论学习\"><a href=\"#理论学习\" class=\"headerlink\" title=\"理论学习\"></a>理论学习</h1><h2 id=\"反向传播算法\"><a href=\"#反向传播算法\" class=\"headerlink\" title=\"反向传播算法\"></a>反向传播算法</h2><p>反向传播是一种基于有监督学习，用于根据误差和损失函数调整网络权重的算法。反向传播算法的核心思想是通过链式法则计算损失函数对于每个权重的梯度，然后使用梯度下降法更新权重。<br>过程：</p>\n<ul>\n<li>首先通过正向传播，根据输入数据得到一个网络的激励</li>\n<li>根据得到的激励与目标值计算损失函数</li>\n<li>根据损失函数，从输出层开始，依次沿着计算图反向计算每个权重的梯度</li>\n<li>根据得到的梯度调整权重<br>[1]\t <a href=\"https://books.google.com/books/about/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8.html?id=2-PWvQEACAAJ\">深度学习入门: 基于Python的理论与实现[M]. 人民邮电出版社, 2018.(p.121,161)</a><h2 id=\"前馈\"><a href=\"#前馈\" class=\"headerlink\" title=\"前馈\"></a>前馈</h2>前馈神经网络是一种最简单的神经网络，它的每个神经元都是前一层神经元的输出。前馈神经网络的每个神经元都是前一层神经元的输出，因此它的输出不会反馈到输入层，这种网络结构也被称为前馈神经网络。</li>\n</ul>\n<h2 id=\"卷积神经网络\"><a href=\"#卷积神经网络\" class=\"headerlink\" title=\"卷积神经网络\"></a>卷积神经网络</h2><h3 id=\"卷积\"><a href=\"#卷积\" class=\"headerlink\" title=\"卷积\"></a>卷积</h3><p>卷积（convolution）是一种数学运算，主要应用于信号处理中对系统响应的计算。卷积运算可以将某个冲激响应针对任意输入进行计算，得到对应的响应结果。卷积运算的公式如下：<br>$$<br>y(t) &#x3D; \\int_{-\\infty}^{\\infty} x(a)h(t-a)da<br>$$<br>其中，$x(t)$为输入信号，$h(t)$为系统响应，$y(t)$为输出信号。</p>\n<h3 id=\"二维离散卷积\"><a href=\"#二维离散卷积\" class=\"headerlink\" title=\"二维离散卷积\"></a>二维离散卷积</h3><p>对于图像处理来说，卷积需要用到二维矩阵的滑动窗口来进行卷积运算。二维离散卷积的公式如下：<br>$$<br>y(i,j) &#x3D; \\sum_{m&#x3D;-\\infty}^{\\infty}\\sum_{n&#x3D;-\\infty}^{\\infty}x(m,n)h(i-m,j-n)<br>$$<br>其中，$x(m,n)$为输入图像，$h(i,j)$为卷积核，$y(i,j)$为输出图像。</p>\n<h3 id=\"卷积神经网络-1\"><a href=\"#卷积神经网络-1\" class=\"headerlink\" title=\"卷积神经网络\"></a>卷积神经网络</h3><p>卷积神经网络（CNN）中，增加了卷积层和池化层。其可以从原本多维度的数据中提取欧氏距离较近的单元之间蕴含的信息。</p>\n<h4 id=\"卷积层\"><a href=\"#卷积层\" class=\"headerlink\" title=\"卷积层\"></a>卷积层</h4><p>在卷积层中，当输入数据是图像时，卷积层会以三维数据形式接收数据，并以三维数据形式传输到下一层，输入输出数据称为特征图（feature map）。其中，卷积层的输入数据称为输入特征图（input feature map），输出数据称为输出特征图（output feature map）。</p>\n<h4 id=\"CNN的处理流\"><a href=\"#CNN的处理流\" class=\"headerlink\" title=\"CNN的处理流\"></a>CNN的处理流</h4><p>针对一个图像，有三维的信息（长、宽、通道），同样，对这个图像进行处理的卷积核也是三维的。但是最终卷积得到的输出结果是二维的（每个通道卷积的结果加在一起）。在CNN中，针对多个卷积核，会得到多个二维的输出结果，这些输出结果会被叠加在一起，得到一个三维的输出结果。这个结果传递给下一层。同时，对多个数据，即批处理，卷积层将多个样本汇总成一次处理，传递中综合成四维的数据。</p>\n<h4 id=\"池化层\"><a href=\"#池化层\" class=\"headerlink\" title=\"池化层\"></a>池化层</h4><p>池化层是一种降低数据大小的方法，它可以减少数据的大小，同时也可以防止过拟合。池化层的处理流程如下：</p>\n<ul>\n<li>按照设定的步长，从输入数据中提取数据块</li>\n<li>例如MAX池化，将数据块中的最大值作为输出结果</li>\n<li>输出结果的规模即随步长变大而缩小<br>同时，池化层输入数据和输出数据的维度相同<h2 id=\"循环神经网络\"><a href=\"#循环神经网络\" class=\"headerlink\" title=\"循环神经网络\"></a>循环神经网络</h2>循环神经网络常用于nlp领域。它和前馈神经网络或CNN的主要区别在于循环神经网络（RNN）的隐藏层的输出不仅仅取决于当前的输入，还取决于前一时刻的隐藏层的输出。因此，RNN具有某种程度上的“记忆”能力。<br>另一个显著特征在于它们在每个网络层共享参数，RNN在每一层都共享相同的参数，这使得它们可以处理任意长度的序列。<br>然而，RNN在反向传播的过程中，梯度会随着时间的推移而消失或爆炸，这使得它们很难学习长期依赖关系。<h2 id=\"注意力机制\"><a href=\"#注意力机制\" class=\"headerlink\" title=\"注意力机制\"></a>注意力机制</h2>注意力机制可以增强神经网络输入数据中某些部分的权重，同时减弱其他部分的权重。<br>例如对一个翻译句子的网络，普通的逐个词翻译会在每一轮翻译过程中对单词序列依次提高注意力，也就是其注意力矩阵会是一个对角线上权值高的矩阵。但是在注意力机制下，每一轮翻译过程中，网络会根据上一轮的翻译结果，对输入句子中的某些部分进行更多的关注，即其权值的最大值不一定在对角线。从而提高翻译的连贯性。<h2 id=\"并行计算\"><a href=\"#并行计算\" class=\"headerlink\" title=\"并行计算\"></a>并行计算</h2>并行计算对计算任务进行拆分，将同时进行的计算任务分配到不同的计算单元上，从而提高计算速度。拆分的方式统称为并行方式，并行计算后的结果重新聚合的方式称为模型更新传递方式。<br>常见的并行方式有：</li>\n<li>数据并行：把数据集切分放到各个计算节点，并在哥哥节点之间传递模型参数</li>\n<li>模型并行：把模型切分放到各个计算节点，并在各个节点之间传递数据。一般把单个算子分配在配置相同的几个硬件上进行模型存储和计算。</li>\n<li>流水线并行：将模型切分成多个阶段，每个阶段在不同的计算节点上进行计算，每个阶段的计算结果传递给下一个阶段。<br>另外，如何更新模型参数也是并行计算的一个重要问题。在硬件组织架构方面，分为参数服务器架构和collective架构。在更新参数方面分为同步和异步更新<a href=\"https://zhuanlan.zhihu.com/p/350501860\">参考内容</a><h3 id=\"allreduce训练\"><a href=\"#allreduce训练\" class=\"headerlink\" title=\"allreduce训练\"></a>allreduce训练</h3>在同步更新参数的训练中，利用AllReduce来整合不同worker的梯度数据。AllReduce有很多种类的实现，主要关注的问题在于不同worker之间传递信息的拓扑结构。例如，对于一个有4个worker的集群，有以下几种拓扑结构：</li>\n<li><strong>ring</strong>：每个worker只和相邻的worker通信</li>\n<li><strong>mesh</strong>：每个worker和所有其他worker通信，但是效率比较低。</li>\n<li><strong>Master-Worker</strong>：一个worker作为master，其他worker作为worker，master和每个worker通信，worker之间不通信。<br>举N个worker的ring结构为例，考察这个结构的工作过程：</li>\n<li>每个worker计算自己的梯度</li>\n<li>每个worker把数据分成N份</li>\n<li>第k个worker把其第k份数据发送给第k+1个worker</li>\n<li>第k个worker把其第k-1份数据和第k-1个worker发送的数据整合，再发给下一个worker</li>\n<li>循环N次之后，每个worker包含最终整合结果的1份</li>\n<li>每个worker把自己的数据发送给下一个worker，收到数据后，每个worker的数据都是最终整合结果<br>这个结构的AllReduce的优势在于发送的数据量是固定的，和worker数量无关，避免了网络拥塞。<a href=\"https://zhuanlan.zhihu.com/p/100012827\">参考内容</a><h1 id=\"实践内容\"><a href=\"#实践内容\" class=\"headerlink\" title=\"实践内容\"></a>实践内容</h1><h2 id=\"lenet5\"><a href=\"#lenet5\" class=\"headerlink\" title=\"lenet5\"></a>lenet5</h2>lenet5是进行手写数字识别的CNN，它的结构如下：<br>输入层-&gt;卷积层-&gt;池化层-&gt;卷积层-&gt;池化层-&gt;全连接层-&gt;全连接层-&gt;输出层（高斯连接）<br>与CNN不同的地方在于，LeNet使用sigmoid函数而非reLU函数。<br>lenet5网络的实现代码如下：<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><div class=\"code-wrapper\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></div></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-class\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title\">LeNet</span>(<span class=\"hljs-params\">nn.Module</span>):</span><br>    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">__init__</span>(<span class=\"hljs-params\">self</span>):</span><br>        <span class=\"hljs-built_in\">super</span>(LeNet, self).__init__()<br>        self.conv = nn.Sequential(<br>            nn.Conv2d(<span class=\"hljs-number\">1</span>, <span class=\"hljs-number\">6</span>, <span class=\"hljs-number\">5</span>), <span class=\"hljs-comment\"># in_channels, out_channels, kernel_size</span><br>            nn.Sigmoid(),<br>            nn.MaxPool2d(<span class=\"hljs-number\">2</span>, <span class=\"hljs-number\">2</span>), <span class=\"hljs-comment\"># kernel_size, stride</span><br>            nn.Conv2d(<span class=\"hljs-number\">6</span>, <span class=\"hljs-number\">16</span>, <span class=\"hljs-number\">5</span>),<br>            nn.Sigmoid(),<br>            nn.MaxPool2d(<span class=\"hljs-number\">2</span>, <span class=\"hljs-number\">2</span>)<br>        )<br>        self.fc = nn.Sequential(<br>            nn.Linear(<span class=\"hljs-number\">16</span>*<span class=\"hljs-number\">4</span>*<span class=\"hljs-number\">4</span>, <span class=\"hljs-number\">120</span>),<br>            nn.Sigmoid(),<br>            nn.Linear(<span class=\"hljs-number\">120</span>, <span class=\"hljs-number\">84</span>),<br>            nn.Sigmoid(),<br>            nn.Linear(<span class=\"hljs-number\">84</span>, <span class=\"hljs-number\">10</span>)<br>        )<br><br>    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">forward</span>(<span class=\"hljs-params\">self, img</span>):</span><br>        feature = self.conv(img)<br>        output = self.fc(feature.view(img.shape[<span class=\"hljs-number\">0</span>], -<span class=\"hljs-number\">1</span>))<br>        <span class=\"hljs-keyword\">return</span> output<br><br></code></pre></td></tr></table></figure>\n这个网络定义了两个部分，一个是卷积层，一个是全连接层。卷积层的输入是一个1通道的图像，输出是一个6通道的图像，卷积核的大小为5*5。全连接层的输入是16*4*4的数据，输出是10个类别的概率。<h2 id=\"resnet\"><a href=\"#resnet\" class=\"headerlink\" title=\"resnet\"></a>resnet</h2>ResNet主要用于解决深度神经网络无法找到更好的解的问题。在深层网络中，梯度消失或爆炸的问题会导致网络无法训练。ResNet通过引入残差块（residual block）来解决这个问题。ResNet将堆叠的几个隐含层作为一个残差块，用残差块拟合的函数从原本的f(x)变为f(x)+x。<br>[4]\t<a href=\"https://arxiv.org/abs/1512.03385\">HE K, ZHANG X, REN S, et al. Deep residual learning for image recognition[C]. Proceedings of the IEEE conference on computer vision and pattern recognition, 2016:770-778.</a><br>通过每个block中残差路径和shortcut路径的设计，可以实现不同的ResNet网络。事实证明，不断增加ResNet的深度，也没有发生解的退化，反而可以提高网络的性能。因此ResNet可以实现如下的网络结构：<br><img src=\"https://raw.githubusercontent.com/lianga1/picGo_test/main/3u8Wwj.png\" alt=\"resnet\"><h3 id=\"实际部署\"><a href=\"#实际部署\" class=\"headerlink\" title=\"实际部署\"></a>实际部署</h3>残差块类定义如下：<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-class\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title\">Bottleneck</span>(<span class=\"hljs-params\">nn.Module</span>):</span><br>    <span class=\"hljs-comment\"># 残差块定义</span><br>    extention = <span class=\"hljs-number\">4</span><br><br>    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">__init__</span>(<span class=\"hljs-params\">self, inplanes, planes, stride, downsample=<span class=\"hljs-literal\">None</span></span>):</span><br>        <span class=\"hljs-built_in\">super</span>(Bottleneck, self).__init__()<br>        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=<span class=\"hljs-number\">1</span>, stride=stride, bias=<span class=\"hljs-literal\">False</span>)<br>        self.bn1 = nn.BatchNorm2d(planes)<br><br>        self.conv2 = nn.Conv2d(planes, planes, kernel_size=<span class=\"hljs-number\">3</span>, stride=<span class=\"hljs-number\">1</span>, padding=<span class=\"hljs-number\">1</span>, bias=<span class=\"hljs-literal\">False</span>)<br>        self.bn2 = nn.BatchNorm2d(planes)<br><br>        self.conv3 = nn.Conv2d(planes, planes * self.extention, kernel_size=<span class=\"hljs-number\">1</span>, stride=<span class=\"hljs-number\">1</span>, bias=<span class=\"hljs-literal\">False</span>)<br>        self.bn3 = nn.BatchNorm2d(planes * self.extention)<br><br>        self.relu = nn.ReLU(inplace=<span class=\"hljs-literal\">True</span>)<br><br>        self.downsample = downsample<br>        self.stride = stride<br></code></pre></td></tr></table></figure>\nResNet网络定义如下：<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-class\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title\">ResNet50</span>(<span class=\"hljs-params\">nn.Module</span>):</span><br>    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">__init__</span>(<span class=\"hljs-params\">self, block, layers, num_class</span>):</span><br>        self.inplane = <span class=\"hljs-number\">64</span><br>        <span class=\"hljs-built_in\">super</span>(ResNet50, self).__init__()<br><br>        self.block = block<br>        self.layers = layers<br><br>        self.conv1 = nn.Conv2d(<span class=\"hljs-number\">3</span>, self.inplane, kernel_size=<span class=\"hljs-number\">7</span>, stride=<span class=\"hljs-number\">2</span>, padding=<span class=\"hljs-number\">3</span>, bias=<span class=\"hljs-literal\">False</span>)<br>        self.bn1 = nn.BatchNorm2d(self.inplane)<br>        self.relu = nn.ReLU()<br>        self.maxpool = nn.MaxPool2d(kernel_size=<span class=\"hljs-number\">3</span>, stride=<span class=\"hljs-number\">2</span>, padding=<span class=\"hljs-number\">1</span>)<br><br>        self.stage1 = self.make_layer(self.block, <span class=\"hljs-number\">64</span>, layers[<span class=\"hljs-number\">0</span>], stride=<span class=\"hljs-number\">1</span>)<br>        self.stage2 = self.make_layer(self.block, <span class=\"hljs-number\">128</span>, layers[<span class=\"hljs-number\">1</span>], stride=<span class=\"hljs-number\">2</span>)<br>        self.stage3 = self.make_layer(self.block, <span class=\"hljs-number\">256</span>, layers[<span class=\"hljs-number\">2</span>], stride=<span class=\"hljs-number\">2</span>)<br>        self.stage4 = self.make_layer(self.block, <span class=\"hljs-number\">512</span>, layers[<span class=\"hljs-number\">3</span>], stride=<span class=\"hljs-number\">2</span>)<br><br>        self.avgpool = nn.AvgPool2d(<span class=\"hljs-number\">7</span>)<br>        self.fc = nn.Linear(<span class=\"hljs-number\">512</span> * block.extention, num_class)<br><br><br></code></pre></td></tr></table></figure>\n在30Epoch后，在测试集的准确度达到了75%。</li>\n</ul>\n<h2 id=\"BERT\"><a href=\"#BERT\" class=\"headerlink\" title=\"BERT\"></a>BERT</h2><p>BERT是基于Transformer的预训练模型，主要用于自然语言处理，它能够预测句子中缺失的词语。以及判断两个句子是不是上下句。<br>整个框架由多层transformer的encoder堆叠而成。encoder由注意力层和feed-forward层组成。<br>BERT中，输入由三种不同embedding组成：</p>\n<ul>\n<li><p>wordpiece embedding：由但词向量组成将单词划分成一组有限公共子词单元。</p>\n</li>\n<li><p>position embedaang：将单词的位置信息编码成特征向量。Transformer通过制定规则来构建一个position embedding</p>\n</li>\n<li><p>segment embedding：用于区分两个句子的向量表示。用于区别问答等非对称子句。</p>\n<h3 id=\"网络结构\"><a href=\"#网络结构\" class=\"headerlink\" title=\"网络结构\"></a>网络结构</h3><p>BERT的主要结构是Transformer，Transformer结构如下图所示：<br><img src=\"https://raw.githubusercontent.com/lianga1/picGo_test/main/20200814234510853.jpg\" alt=\"transformer\"><br>其中左侧部分即为encoder部分。<br>encoder单元由一个multi-head-Attention + Layer Normalization + feedforword + Layer Normalization 叠加产生。<br>在比较大的BERT模型中，有24层encoder，每层有16个Attention，词向量维度1024。在较小情况下，有12层encoder，每层12个Attention，词向量维度768。<br>任何时候feed-forward大小都是词向量维度的4倍。</p>\n<h4 id=\"Attention-Layer\"><a href=\"#Attention-Layer\" class=\"headerlink\" title=\"Attention Layer\"></a>Attention Layer</h4><p>这一层的输入是由X &#x3D; (batch_size,max_len_embedding)构成的。<br>单个self-attention 计算过程是输入X分别和三个矩阵Wq,Wk,Wv相乘，得到Q,K,V。然后计算Q和K的点积，再除以$\\sqrt{d_k}$，再经过softmax函数，得到attention矩阵。最后将attention矩阵和V相乘即加权求和，得到输出。<br>multi-head-Attention将多个不同的self-attention输出进行拼接，然后再乘以一个矩阵W0，得到最终的输出output_sum &#x3D; (batch_size,max_len,n*w_length)这个结果再经过一个全连接层就是整个multi-head-Attention的输出。</p>\n<h4 id=\"Layer-Normalization\"><a href=\"#Layer-Normalization\" class=\"headerlink\" title=\"Layer Normalization\"></a>Layer Normalization</h4><p>这个层相当于对每句话的embedding做归一化，所以用LN而非Batch Normalization</p>\n<h4 id=\"BERT每一层的学习\"><a href=\"#BERT每一层的学习\" class=\"headerlink\" title=\"BERT每一层的学习\"></a>BERT每一层的学习</h4><p>从浅层到深层分别可以学习到surface，短语，语法和语义的信息。</p>\n<h3 id=\"BERT的训练\"><a href=\"#BERT的训练\" class=\"headerlink\" title=\"BERT的训练\"></a>BERT的训练</h3><p>定义几个层的类如下：</p>\n</li>\n<li><p>Embedding：输入的embedding层，包括wordpiece embedding，position embedding，segment embedding</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-class\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title\">Embeddings</span>(<span class=\"hljs-params\">nn.Module</span>):</span><br>    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">__init__</span>(<span class=\"hljs-params\">self</span>):</span><br>        <span class=\"hljs-built_in\">super</span>(Embeddings, self).__init__()<br>        self.seg_emb = nn.Embedding(n_segs, d_model)<br>        self.word_emb = nn.Embedding(max_vocab, d_model)<br>        self.pos_emb = nn.Embedding(max_len, d_model)<br>        self.norm = nn.LayerNorm(d_model)<br>        self.dropout = nn.Dropout(p_dropout)<br><br>    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">forward</span>(<span class=\"hljs-params\">self, x, seg</span>):</span><br>        <span class=\"hljs-string\">&#x27;&#x27;&#x27;</span><br><span class=\"hljs-string\">        x: [batch, seq_len]</span><br><span class=\"hljs-string\">        &#x27;&#x27;&#x27;</span><br>        word_enc = self.word_emb(x)<br><br>        <span class=\"hljs-comment\"># positional embedding</span><br>        pos = torch.arange(x.shape[<span class=\"hljs-number\">1</span>], dtype=torch.long, device=device)<br>        pos = pos.unsqueeze(<span class=\"hljs-number\">0</span>).expand_as(x)<br>        pos_enc = self.pos_emb(pos)<br><br>        seg_enc = self.seg_emb(seg)<br>        x = self.norm(word_enc + pos_enc + seg_enc)<br>        <span class=\"hljs-keyword\">return</span> self.dropout(x)<br>        <span class=\"hljs-comment\"># return: [batch, seq_len, d_model]</span><br></code></pre></td></tr></table></figure>\n</li>\n<li><p>Multi-Head-Attention层</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-class\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title\">ScaledDotProductAttention</span>(<span class=\"hljs-params\">nn.Module</span>):</span><br>    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">__init__</span>(<span class=\"hljs-params\">self</span>):</span><br>        <span class=\"hljs-built_in\">super</span>(ScaledDotProductAttention, self).__init__()<br><br>    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">forward</span>(<span class=\"hljs-params\">self, Q, K, V, attn_mask</span>):</span><br>        scores = torch.matmul(Q, K.transpose(-<span class=\"hljs-number\">1</span>, -<span class=\"hljs-number\">2</span>) / msqrt(d_k))<br>        <span class=\"hljs-comment\"># scores: [batch, n_heads, seq_len, seq_len]</span><br>        scores.masked_fill_(attn_mask, -<span class=\"hljs-number\">1e9</span>)<br>        attn = nn.Softmax(dim=-<span class=\"hljs-number\">1</span>)(scores)<br>        <span class=\"hljs-comment\"># context: [batch, n_heads, seq_len, d_v]</span><br>        context = torch.matmul(attn, V)<br>        <span class=\"hljs-keyword\">return</span> context<br><br><span class=\"hljs-class\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title\">MultiHeadAttention</span>(<span class=\"hljs-params\">nn.Module</span>):</span><br>    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">__init__</span>(<span class=\"hljs-params\">self</span>):</span><br>        <span class=\"hljs-built_in\">super</span>(MultiHeadAttention, self).__init__()<br>        self.W_Q = nn.Linear(d_model, d_k * n_heads, bias=<span class=\"hljs-literal\">False</span>)<br>        self.W_K = nn.Linear(d_model, d_k * n_heads, bias=<span class=\"hljs-literal\">False</span>)<br>        self.W_V = nn.Linear(d_model, d_v * n_heads, bias=<span class=\"hljs-literal\">False</span>)<br>        self.fc = nn.Linear(n_heads * d_v, d_model, bias=<span class=\"hljs-literal\">False</span>)<br><br>    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">forward</span>(<span class=\"hljs-params\">self, Q, K, V, attn_mask</span>):</span><br>        <span class=\"hljs-string\">&#x27;&#x27;&#x27;</span><br><span class=\"hljs-string\">        Q, K, V: [batch, seq_len, d_model]</span><br><span class=\"hljs-string\">        attn_mask: [batch, seq_len, seq_len]</span><br><span class=\"hljs-string\">        &#x27;&#x27;&#x27;</span><br>        batch = Q.size(<span class=\"hljs-number\">0</span>)<br>        <span class=\"hljs-string\">&#x27;&#x27;&#x27;</span><br><span class=\"hljs-string\">        split Q, K, V to per head formula: [batch, seq_len, n_heads, d_k]</span><br><span class=\"hljs-string\">        Convenient for matrix multiply opearation later</span><br><span class=\"hljs-string\">        q, k, v: [batch, n_heads, seq_len, d_k / d_v]</span><br><span class=\"hljs-string\">        &#x27;&#x27;&#x27;</span><br>        per_Q = self.W_Q(Q).view(batch, -<span class=\"hljs-number\">1</span>, n_heads, d_k).transpose(<span class=\"hljs-number\">1</span>, <span class=\"hljs-number\">2</span>)<br>        per_K = self.W_K(K).view(batch, -<span class=\"hljs-number\">1</span>, n_heads, d_k).transpose(<span class=\"hljs-number\">1</span>, <span class=\"hljs-number\">2</span>)<br>        per_V = self.W_V(V).view(batch, -<span class=\"hljs-number\">1</span>, n_heads, d_v).transpose(<span class=\"hljs-number\">1</span>, <span class=\"hljs-number\">2</span>)<br><br>        attn_mask = attn_mask.unsqueeze(<span class=\"hljs-number\">1</span>).repeat(<span class=\"hljs-number\">1</span>, n_heads, <span class=\"hljs-number\">1</span>, <span class=\"hljs-number\">1</span>)<br>        <span class=\"hljs-comment\"># context: [batch, n_heads, seq_len, d_v]</span><br>        context = ScaledDotProductAttention()(per_Q, per_K, per_V, attn_mask)<br>        context = context.transpose(<span class=\"hljs-number\">1</span>, <span class=\"hljs-number\">2</span>).contiguous().view(<br>            batch, -<span class=\"hljs-number\">1</span>, n_heads * d_v)<br><br>        <span class=\"hljs-comment\"># output: [batch, seq_len, d_model]</span><br>        output = self.fc(context)<br>        <span class=\"hljs-keyword\">return</span> output<br></code></pre></td></tr></table></figure></li>\n<li><p>其余层，包括FeedForword层和池化层</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-class\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title\">FeedForwardNetwork</span>(<span class=\"hljs-params\">nn.Module</span>):</span><br>    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">__init__</span>(<span class=\"hljs-params\">self</span>):</span><br>        <span class=\"hljs-built_in\">super</span>(FeedForwardNetwork, self).__init__()<br>        self.fc1 = nn.Linear(d_model, d_ff)<br>        self.fc2 = nn.Linear(d_ff, d_model)<br>        self.dropout = nn.Dropout(p_dropout)<br>        self.gelu = gelu<br><br>    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">forward</span>(<span class=\"hljs-params\">self, x</span>):</span><br>        x = self.fc1(x)<br>        x = self.dropout(x)<br>        x = self.gelu(x)<br>        x = self.fc2(x)<br>        <span class=\"hljs-keyword\">return</span> x<br><br><span class=\"hljs-class\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title\">Pooler</span>(<span class=\"hljs-params\">nn.Module</span>):</span><br>    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">__init__</span>(<span class=\"hljs-params\">self</span>):</span><br>        <span class=\"hljs-built_in\">super</span>(Pooler, self).__init__()<br>        self.fc = nn.Linear(d_model, d_model)<br>        self.tanh = nn.Tanh()<br><br>    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">forward</span>(<span class=\"hljs-params\">self, x</span>):</span><br>        <span class=\"hljs-string\">&#x27;&#x27;&#x27;</span><br><span class=\"hljs-string\">        x: [batch, d_model] (first place output)</span><br><span class=\"hljs-string\">        &#x27;&#x27;&#x27;</span><br>        x = self.fc(x)<br>        x = self.tanh(x)<br>        <span class=\"hljs-keyword\">return</span> x<br><br></code></pre></td></tr></table></figure></li>\n<li><p>Encoder层和组合而成的BERT网络</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-class\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title\">EncoderLayer</span>(<span class=\"hljs-params\">nn.Module</span>):</span><br>    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">__init__</span>(<span class=\"hljs-params\">self</span>):</span><br>        <span class=\"hljs-built_in\">super</span>(EncoderLayer, self).__init__()<br>        self.norm1 = nn.LayerNorm(d_model)<br>        self.norm2 = nn.LayerNorm(d_model)<br><br>        self.enc_attn = MultiHeadAttention()<br>        self.ffn = FeedForwardNetwork()<br><br>    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">forward</span>(<span class=\"hljs-params\">self, x, pad_mask</span>):</span><br>        <span class=\"hljs-string\">&#x27;&#x27;&#x27;</span><br><span class=\"hljs-string\">        pre-norm</span><br><span class=\"hljs-string\">        see more detail in https://openreview.net/pdf?id=B1x8anVFPr</span><br><span class=\"hljs-string\"></span><br><span class=\"hljs-string\">        x: [batch, seq_len, d_model]</span><br><span class=\"hljs-string\">        &#x27;&#x27;&#x27;</span><br>        residual = x<br>        x = self.norm1(x)<br>        x = self.enc_attn(x, x, x, pad_mask) + residual<br>        residual = x<br>        x = self.norm2(x)<br>        x = self.ffn(x)<br>        <span class=\"hljs-keyword\">return</span> x + residual<br><br><br><span class=\"hljs-class\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title\">BERT</span>(<span class=\"hljs-params\">nn.Module</span>):</span><br>    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">__init__</span>(<span class=\"hljs-params\">self, n_layers</span>):</span><br>        <span class=\"hljs-built_in\">super</span>(BERT, self).__init__()<br>        self.embedding = Embeddings()<br>        self.encoders = nn.ModuleList([<br>            EncoderLayer() <span class=\"hljs-keyword\">for</span> _ <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">range</span>(n_layers)<br>        ])<br><br>        self.pooler = Pooler()<br><br>        self.next_cls = nn.Linear(d_model, <span class=\"hljs-number\">2</span>)<br>        self.gelu = gelu<br><br>        shared_weight = self.pooler.fc.weight<br>        self.fc = nn.Linear(d_model, d_model)<br>        self.fc.weight = shared_weight<br><br>        shared_weight = self.embedding.word_emb.weight<br>        self.word_classifier = nn.Linear(d_model, max_vocab, bias=<span class=\"hljs-literal\">False</span>)<br>        self.word_classifier.weight = shared_weight<br><br>    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">forward</span>(<span class=\"hljs-params\">self, tokens, segments, masked_pos</span>):</span><br>        output = self.embedding(tokens, segments)<br>        enc_self_pad_mask = get_pad_mask(tokens)<br>        <span class=\"hljs-keyword\">for</span> layer <span class=\"hljs-keyword\">in</span> self.encoders:<br>            output = layer(output, enc_self_pad_mask)<br>        <span class=\"hljs-comment\"># output: [batch, max_len, d_model]</span><br><br>        <span class=\"hljs-comment\"># NSP Task</span><br>        hidden_pool = self.pooler(output[:, <span class=\"hljs-number\">0</span>])<br>        logits_cls = self.next_cls(hidden_pool)<br><br>        <span class=\"hljs-comment\"># Masked Language Model Task</span><br>        <span class=\"hljs-comment\"># masked_pos: [batch, max_pred] -&gt; [batch, max_pred, d_model]</span><br>        masked_pos = masked_pos.unsqueeze(-<span class=\"hljs-number\">1</span>).expand(-<span class=\"hljs-number\">1</span>, -<span class=\"hljs-number\">1</span>, d_model)<br><br>        <span class=\"hljs-comment\"># h_masked: [batch, max_pred, d_model]</span><br>        h_masked = torch.gather(output, dim=<span class=\"hljs-number\">1</span>, index=masked_pos)<br>        h_masked = self.gelu(self.fc(h_masked))<br>        logits_lm = self.word_classifier(h_masked)<br>        <span class=\"hljs-comment\"># logits_lm: [batch, max_pred, max_vocab]</span><br>        <span class=\"hljs-comment\"># logits_cls: [batch, 2]</span><br><br>        <span class=\"hljs-keyword\">return</span> logits_cls, logits_lm<br><br></code></pre></td></tr></table></figure>\n<p>batch-size设为6<br>训练300个Epoch<br>训练结果进行预测例句<br>结果如下：</p>\n<figure class=\"highlight routeros\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs routeros\">========================================================<br>Masked data:<br>[<span class=\"hljs-string\">&#x27;[CLS]&#x27;</span>, <span class=\"hljs-string\">&#x27;[MASK]&#x27;</span>, <span class=\"hljs-string\">&#x27;[MASK]&#x27;</span>, <span class=\"hljs-string\">&#x27;you&#x27;</span>, <span class=\"hljs-string\">&#x27;too&#x27;</span>, <span class=\"hljs-string\">&#x27;how&#x27;</span>, <span class=\"hljs-string\">&#x27;are&#x27;</span>, <span class=\"hljs-string\">&#x27;you&#x27;</span>, <span class=\"hljs-string\">&#x27;today&#x27;</span>, <span class=\"hljs-string\">&#x27;[SEP]&#x27;</span>, <span class=\"hljs-string\">&#x27;great&#x27;</span>,<br> <span class=\"hljs-string\">&#x27;my&#x27;</span>, <span class=\"hljs-string\">&#x27;baseball&#x27;</span>, <span class=\"hljs-string\">&#x27;team&#x27;</span>, <span class=\"hljs-string\">&#x27;won&#x27;</span>, <span class=\"hljs-string\">&#x27;the&#x27;</span>, <span class=\"hljs-string\">&#x27;competition&#x27;</span>, <span class=\"hljs-string\">&#x27;[SEP]&#x27;</span>]<br>BERT reconstructed:<br>[<span class=\"hljs-string\">&#x27;[CLS]&#x27;</span>, <span class=\"hljs-string\">&#x27;nice&#x27;</span>, <span class=\"hljs-string\">&#x27;meet&#x27;</span>, <span class=\"hljs-string\">&#x27;you&#x27;</span>, <span class=\"hljs-string\">&#x27;too&#x27;</span>, <span class=\"hljs-string\">&#x27;how&#x27;</span>, <span class=\"hljs-string\">&#x27;are&#x27;</span>, <span class=\"hljs-string\">&#x27;you&#x27;</span>, <span class=\"hljs-string\">&#x27;today&#x27;</span>, <span class=\"hljs-string\">&#x27;[SEP]&#x27;</span>, <span class=\"hljs-string\">&#x27;great&#x27;</span>,<br> <span class=\"hljs-string\">&#x27;my&#x27;</span>, <span class=\"hljs-string\">&#x27;baseball&#x27;</span>, <span class=\"hljs-string\">&#x27;team&#x27;</span>, <span class=\"hljs-string\">&#x27;won&#x27;</span>, <span class=\"hljs-string\">&#x27;the&#x27;</span>, <span class=\"hljs-string\">&#x27;competition&#x27;</span>, <span class=\"hljs-string\">&#x27;[SEP]&#x27;</span>]<br>Original sentence:<br>[<span class=\"hljs-string\">&#x27;[CLS]&#x27;</span>, <span class=\"hljs-string\">&#x27;nice&#x27;</span>, <span class=\"hljs-string\">&#x27;meet&#x27;</span>, <span class=\"hljs-string\">&#x27;you&#x27;</span>, <span class=\"hljs-string\">&#x27;too&#x27;</span>, <span class=\"hljs-string\">&#x27;how&#x27;</span>, <span class=\"hljs-string\">&#x27;are&#x27;</span>, <span class=\"hljs-string\">&#x27;you&#x27;</span>, <span class=\"hljs-string\">&#x27;today&#x27;</span>, <span class=\"hljs-string\">&#x27;[SEP]&#x27;</span>, <span class=\"hljs-string\">&#x27;great&#x27;</span>,<br> <span class=\"hljs-string\">&#x27;my&#x27;</span>, <span class=\"hljs-string\">&#x27;baseball&#x27;</span>, <span class=\"hljs-string\">&#x27;team&#x27;</span>, <span class=\"hljs-string\">&#x27;won&#x27;</span>, <span class=\"hljs-string\">&#x27;the&#x27;</span>, <span class=\"hljs-string\">&#x27;competition&#x27;</span>, <span class=\"hljs-string\">&#x27;[SEP]&#x27;</span>]<br>===============Next Sentence <span class=\"hljs-attribute\">Prediction</span>===============<br>Two sentences are continuous? <span class=\"hljs-literal\">True</span><br>BERT predict: <span class=\"hljs-literal\">True</span><br></code></pre></td></tr></table></figure></li>\n</ul>\n",
            "tags": [
                "技术",
                "周报",
                "课题组",
                "神经网络",
                "pytorch"
            ]
        }
    ]
}