<?xml version="1.0"?>
<rss version="2.0">
    <channel>
        <title>意大利炮打友军 • Posts by &#34;通信&#34; tag</title>
        <link>http://example.com</link>
        <description></description>
        <language>zh-CN</language>
        <pubDate>Tue, 26 Mar 2024 19:54:58 +0800</pubDate>
        <lastBuildDate>Tue, 26 Mar 2024 19:54:58 +0800</lastBuildDate>
        <category>随笔</category>
        <category>技术</category>
        <category>大模型训练</category>
        <category>课题组</category>
        <category>笔记</category>
        <category>博客</category>
        <category>markdown</category>
        <category>Linux</category>
        <category>月历</category>
        <category>写作</category>
        <category>科幻</category>
        <category>世界观</category>
        <category>python</category>
        <category>WSL</category>
        <category>编译</category>
        <category>通信</category>
        <category>操作系统</category>
        <category>电赛</category>
        <category>周报</category>
        <category>神经网络</category>
        <category>pytorch</category>
        <category>记录</category>
        <item>
            <guid isPermalink="true">http://example.com/2024/03/26/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%80%9A%E4%BF%A1%E7%AC%94%E8%AE%B02/</guid>
            <title>大模型通信笔记2</title>
            <link>http://example.com/2024/03/26/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%80%9A%E4%BF%A1%E7%AC%94%E8%AE%B02/</link>
            <category>技术</category>
            <category>大模型训练</category>
            <category>博客</category>
            <category>通信</category>
            <pubDate>Tue, 26 Mar 2024 19:54:58 +0800</pubDate>
            <description><![CDATA[ &lt;h2 id=&#34;数据并行&#34;&gt;&lt;a href=&#34;#数据并行&#34; class=&#34;headerlink&#34; title=&#34;数据并行&#34;&gt;&lt;/a&gt;数据并行&lt;/h2&gt;&lt;p&gt;传统的数据并行是让每个GPU分别进行FWD和BWD，然后把梯度进行聚合操作，然后再下发给每个GPU，称为All Reduce。&lt;/p&gt;
&lt;h3 id=&#34;缺点&#34;&gt;&lt;a href=&#34;#缺点&#34; class=&#34;headerlink&#34; title=&#34;缺点&#34;&gt;&lt;/a&gt;缺点&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;存储开销大。每块GPU上都存了一份完整的模型，造成冗余&lt;/li&gt;
&lt;li&gt;通讯开销大。Server需要和每一个Worker进行梯度传输。当Server和Worker不在一台机器上时，Server的带宽将会成为整个系统的计算效率瓶颈。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;异步梯度更新&#34;&gt;&lt;a href=&#34;#异步梯度更新&#34; class=&#34;headerlink&#34; title=&#34;异步梯度更新&#34;&gt;&lt;/a&gt;异步梯度更新&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Worker不等待梯度更新，用旧的参数进行下一轮训练，可能会延迟一步更新梯度，整体收敛速度变慢，但是提升通讯计算比。&lt;/li&gt;
&lt;li&gt;延迟步数会指定&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;分布式数据并行&#34;&gt;&lt;a href=&#34;#分布式数据并行&#34; class=&#34;headerlink&#34; title=&#34;分布式数据并行&#34;&gt;&lt;/a&gt;分布式数据并行&lt;/h3&gt;&lt;p&gt;核心目标是降低通信压力，因此要将Server的通信压力转到Worker上，最简单的就是Ring-AllReduce。&lt;/p&gt;
&lt;h4 id=&#34;Ring-Allreduce&#34;&gt;&lt;a href=&#34;#Ring-Allreduce&#34; class=&#34;headerlink&#34; title=&#34;Ring Allreduce&#34;&gt;&lt;/a&gt;Ring Allreduce&lt;/h4&gt;&lt;p&gt;核心思路是实现Reduce Scatter和All-Gather。GPU每次之和前后两个GPU通信，1卡给2卡发a号数据，2给3发b号，以此类推。三次更新后每张卡都有1个号的完整的数据。&lt;/p&gt;
&lt;p&gt;之后在进行All-Gather，依旧环形通信，把每个部分全聚合的都发给下一个，然后依此类推，3轮通信就可以覆盖所有。&lt;/p&gt;
&lt;h2 id=&#34;显存开销&#34;&gt;&lt;a href=&#34;#显存开销&#34; class=&#34;headerlink&#34; title=&#34;显存开销&#34;&gt;&lt;/a&gt;显存开销&lt;/h2&gt;&lt;p&gt;数据并行中，每个卡都存储了所有参数，怎么办？&lt;/p&gt;
&lt;p&gt;在实际存储中，分为两部分存储：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;模型状态：包括参数，优化器，梯度等&lt;/li&gt;
&lt;li&gt;驻留数据：包括activation，碎片内存和缓冲区等。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;优化措施&#34;&gt;&lt;a href=&#34;#优化措施&#34; class=&#34;headerlink&#34; title=&#34;优化措施&#34;&gt;&lt;/a&gt;优化措施&lt;/h2&gt;&lt;h3 id=&#34;混合精度训练&#34;&gt;&lt;a href=&#34;#混合精度训练&#34; class=&#34;headerlink&#34; title=&#34;混合精度训练&#34;&gt;&lt;/a&gt;混合精度训练&lt;/h3&gt;&lt;p&gt;对于参数，activation，梯度，都使用fp16，对于参数（多存一份）和优化器使用fp32。&lt;br&gt;模型必存数据为$K\phi$,那么最终总存储数据为$K\phi + 4\phi$&lt;br&gt;实际上，activation大小和batch有关，而且是可以抛弃的。&lt;/p&gt;
&lt;h3 id=&#34;ZeRO-DP&#34;&gt;&lt;a href=&#34;#ZeRO-DP&#34; class=&#34;headerlink&#34; title=&#34;ZeRO-DP&#34;&gt;&lt;/a&gt;ZeRO-DP&lt;/h3&gt;&lt;h4 id=&#34;第一步：优化器分割&#34;&gt;&lt;a href=&#34;#第一步：优化器分割&#34; class=&#34;headerlink&#34; title=&#34;第一步：优化器分割&#34;&gt;&lt;/a&gt;第一步：优化器分割&lt;/h4&gt;&lt;p&gt;每张卡只存储一部分优化器参数，在数据并行中，先通过AllReduce得到完整梯度，每个卡都更新自己的一部分梯度和参数，然后再AllGather。产生单卡通讯量$\phi$。&lt;/p&gt;
&lt;h4 id=&#34;第二步：梯度分割&#34;&gt;&lt;a href=&#34;#第二步：梯度分割&#34; class=&#34;headerlink&#34; title=&#34;第二步：梯度分割&#34;&gt;&lt;/a&gt;第二步：梯度分割&lt;/h4&gt;&lt;p&gt;经过FWD和BWD后，对梯度进行Reduce-Scatter，保证每张卡都有自己一份聚合梯度，用分割的优化器和梯度进行更新相应的W，然后再AllGather参数进行更新&lt;/p&gt;
&lt;h4 id=&#34;第三步：参数分割&#34;&gt;&lt;a href=&#34;#第三步：参数分割&#34; class=&#34;headerlink&#34; title=&#34;第三步：参数分割&#34;&gt;&lt;/a&gt;第三步：参数分割&lt;/h4&gt;&lt;p&gt;FWD时，先All Gather一次参数，用完即弃。&lt;br&gt;BWD时，再All Gather一次参数，用完即弃&lt;br&gt;用自己的梯度进行一次All Gather得到完整梯度&lt;br&gt;更新参数，无需通信。&lt;/p&gt;
&lt;h3 id=&#34;ZeRO-R&#34;&gt;&lt;a href=&#34;#ZeRO-R&#34; class=&#34;headerlink&#34; title=&#34;ZeRO-R&#34;&gt;&lt;/a&gt;ZeRO-R&lt;/h3&gt;&lt;p&gt;通过对驻留数据进行优化来实现显存使用减少和通信负载降低。&lt;/p&gt;
&lt;h4 id=&#34;activation&#34;&gt;&lt;a href=&#34;#activation&#34; class=&#34;headerlink&#34; title=&#34;activation&#34;&gt;&lt;/a&gt;activation&lt;/h4&gt;&lt;p&gt;每块GPU上只维护部分的activation，需要时再聚合。或者重新计算。&lt;/p&gt;
&lt;h4 id=&#34;Buffer&#34;&gt;&lt;a href=&#34;#Buffer&#34; class=&#34;headerlink&#34; title=&#34;Buffer&#34;&gt;&lt;/a&gt;Buffer&lt;/h4&gt;&lt;p&gt;通过使用固定大小的Buffer，降低通信次数，减少碎片信息发送，提高带宽利用率&lt;/p&gt;
&lt;h4 id=&#34;碎片内存整合&#34;&gt;&lt;a href=&#34;#碎片内存整合&#34; class=&#34;headerlink&#34; title=&#34;碎片内存整合&#34;&gt;&lt;/a&gt;碎片内存整合&lt;/h4&gt;&lt;h3 id=&#34;ZeRO-Offload&#34;&gt;&lt;a href=&#34;#ZeRO-Offload&#34; class=&#34;headerlink&#34; title=&#34;ZeRO-Offload&#34;&gt;&lt;/a&gt;ZeRO-Offload&lt;/h3&gt;&lt;p&gt;见论文，主要是把显存的优化器参数卸载到CPU内存。&lt;/p&gt;
 ]]></description>
        </item>
        <item>
            <guid isPermalink="true">http://example.com/2024/03/26/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%80%9A%E4%BF%A1%E7%AC%94%E8%AE%B01/</guid>
            <title>大模型通信笔记1</title>
            <link>http://example.com/2024/03/26/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%80%9A%E4%BF%A1%E7%AC%94%E8%AE%B01/</link>
            <category>技术</category>
            <category>大模型训练</category>
            <category>博客</category>
            <category>通信</category>
            <pubDate>Tue, 26 Mar 2024 17:38:32 +0800</pubDate>
            <description><![CDATA[ &lt;h2 id=&#34;流水线并行&#34;&gt;&lt;a href=&#34;#流水线并行&#34; class=&#34;headerlink&#34; title=&#34;流水线并行&#34;&gt;&lt;/a&gt;流水线并行&lt;/h2&gt;&lt;h3 id=&#34;朴素层并行&#34;&gt;&lt;a href=&#34;#朴素层并行&#34; class=&#34;headerlink&#34; title=&#34;朴素层并行&#34;&gt;&lt;/a&gt;朴素层并行&lt;/h3&gt;&lt;p&gt;朴素层并行，将模型拆分为多个层，放在不同的GPU上执行&lt;br&gt;但是问题很明显：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GPU利用率低：任意时刻只有一个GPU在工作，其他GPU都在等待结果&lt;/li&gt;
&lt;li&gt;计算和通信没有重叠&lt;/li&gt;
&lt;li&gt;显存占用高，GPU1需要保存所有激活。等待参数更新完成&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;GPipe&#34;&gt;&lt;a href=&#34;#GPipe&#34; class=&#34;headerlink&#34; title=&#34;GPipe&#34;&gt;&lt;/a&gt;GPipe&lt;/h3&gt;&lt;p&gt;Gpipe将整个&lt;strong&gt;minibatch分为4个microbatch&lt;/strong&gt;，然后由GPU0进行计算，之后每个microbatch计算完直接传递给GPU1，以此类推，进行整个前向、反向传播。&lt;br&gt;假设pipeline深度n，microbatch数量m，那么浪费的时间占比为：&lt;br&gt;$$&lt;br&gt;1-\frac{m}{m+n-1}&lt;br&gt;$$&lt;br&gt;所以需要增加microbatch数量m&lt;br&gt;Gpipe在计算过程中，把中间激活用完即弃，因此节省了显存，但是增加了计算代价。&lt;/p&gt;
&lt;h3 id=&#34;PipeDream&#34;&gt;&lt;a href=&#34;#PipeDream&#34; class=&#34;headerlink&#34; title=&#34;PipeDream&#34;&gt;&lt;/a&gt;PipeDream&lt;/h3&gt;&lt;p&gt;PipeDream在GPipe的基础上，在每个microbatch前向结束后就开始反向传播，节省了一些显存，bubble和Gpipe是一样的&lt;/p&gt;
&lt;h3 id=&#34;数据并行可以和流水线并行同时进行&#34;&gt;&lt;a href=&#34;#数据并行可以和流水线并行同时进行&#34; class=&#34;headerlink&#34; title=&#34;数据并行可以和流水线并行同时进行&#34;&gt;&lt;/a&gt;数据并行可以和流水线并行同时进行&lt;/h3&gt;&lt;p&gt;对任意给定GPU，有两个通信部份，一部分包含所有相同层GPU进行All_Reduce(数据并行)。另一部分和上下层进行通信（流水线）。&lt;/p&gt;
&lt;h2 id=&#34;张量并行&#34;&gt;&lt;a href=&#34;#张量并行&#34; class=&#34;headerlink&#34; title=&#34;张量并行&#34;&gt;&lt;/a&gt;张量并行&lt;/h2&gt;&lt;p&gt;张量并行分为两种情况：&lt;strong&gt;列划分&lt;/strong&gt;和&lt;strong&gt;行划分&lt;/strong&gt;&lt;br&gt;列划分：&lt;br&gt;$$&lt;br&gt;XA &amp;#x3D; X[A_1,A_2···A_n]&amp;#x3D;[XA_1,XA_2,···,XA_n]&lt;br&gt;$$&lt;/p&gt;
&lt;p&gt;行划分：&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;\mathbf{x}*A &amp;#x3D; \begin{bmatrix} x_1 &amp;amp; x_2 &amp;amp; \cdots &amp;amp; x_n \end{bmatrix} * \begin{bmatrix}A_1\A_2\A_3\··· \A_n\end{bmatrix}&amp;#x3D;X_1A_1+X_2A_2+X_3A_3···&lt;br&gt;$$&lt;/p&gt;
&lt;p&gt;对列并行来说，由于GeLU函数并不是线性的，因此需要在输出前进行一次通信来合并。&lt;/p&gt;
&lt;h3 id=&#34;2D并行&#34;&gt;&lt;a href=&#34;#2D并行&#34; class=&#34;headerlink&#34; title=&#34;2D并行&#34;&gt;&lt;/a&gt;2D并行&lt;/h3&gt;&lt;p&gt;具体来说，两个矩阵的结果仍然需要串行的计算。但是，单个矩阵中的4个子矩阵可以使用2*2的处理器来并行计算。&lt;/p&gt;
&lt;h3 id=&#34;2-5D并行&#34;&gt;&lt;a href=&#34;#2-5D并行&#34; class=&#34;headerlink&#34; title=&#34;2.5D并行&#34;&gt;&lt;/a&gt;2.5D并行&lt;/h3&gt;&lt;p&gt;这个就是在2D并行的基础上，左矩阵为两个2*2矩阵垂直拼接，那么这两个矩阵是可以分开计算的，所以可以8处理器并行计算。&lt;/p&gt;
&lt;h2 id=&#34;3D并行&#34;&gt;&lt;a href=&#34;#3D并行&#34; class=&#34;headerlink&#34; title=&#34;3D并行&#34;&gt;&lt;/a&gt;3D并行&lt;/h2&gt;&lt;p&gt;流水线+数据+张量并行&lt;/p&gt;
&lt;p&gt;首先，每个节点8个GPU，共两个节点&lt;/p&gt;
&lt;p&gt;8个GPU，分为两组，每组负责一个Layer，一共四个组进行流水线并行。&lt;br&gt;每个组内，用两张卡进行张量并行，一个组分为两个张量小组。一个张量小组负责一个具体的张量运算。&lt;br&gt;对于两个张量小组之间，分享同一个batch不同的数据，在计算结束后两个小组之间要进行all reduce通信。&lt;/p&gt;
 ]]></description>
        </item>
    </channel>
</rss>
